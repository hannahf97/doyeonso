#!/usr/bin/env python3
"""
P&ID ì „ë¬¸ê°€ ì±—ë´‡ ëª¨ë¸ - Streamlit ì—°ë™ìš©
"""

import os
import pickle
import torch
import json
from sentence_transformers import SentenceTransformer
from utils.rag_system_kiwi import RAGSystemWithKiwi
from openai import OpenAI
from loguru import logger
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
from config.database_config import get_db_connection
# ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ import ì¶”ê°€
from PIL import Image, ImageDraw, ImageFont
import io
import base64
from utils.visualize_data import FirstDatasetVisualizer
from io import BytesIO

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class PIDExpertChatbot:
    """P&ID ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡"""
    
    def __init__(self):
        """ì±—ë´‡ ì´ˆê¸°í™”"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            self.client = OpenAI(api_key=self.openai_api_key)
        else:
            self.client = None
            logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.kiwi_rag = RAGSystemWithKiwi()
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
        self.chunks = None
        self.embeddings = None
        
        # ëŒ€í™” ê¸°ë¡
        self.conversation_history = []
        
        # ìƒíƒœ ì €ì¥ ê²½ë¡œ
        self.STATE_PATH = "./state/"
        
        # ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ì •ì˜
        self.expert_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID(Piping and Instrumentation Diagram) ì „ë¬¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- í™”í•™ê³µì • ì„¤ê³„ ë° ì œì–´ì‹œìŠ¤í…œ
- ê³„ì¸¡ê¸°ê¸° ë° ì œì–´ë£¨í”„ ë¶„ì„
- ê³µì • ì•ˆì „ ë° ì´ìƒìƒí™© ëŒ€ì‘
- P&ID ë„ë©´ í•´ì„ ë° ê²€í† 

**ì‘ë‹µ ìŠ¤íƒ€ì¼:**
- ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì¡°ì–¸ ì œê³µ
- ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤
- êµ¬ì²´ì ì¸ ê³„ì¸¡ê¸°ê¸° íƒœê·¸(FT, FC, PT ë“±) ì–¸ê¸‰
- í•„ìš”ì‹œ ì¶”ê°€ ê²€í† ì‚¬í•­ ì œì•ˆ

**ì‘ë‹µ êµ¬ì¡°:**
1. í•µì‹¬ ë‹µë³€ (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)
2. ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ (ê´€ë ¨ ê³„ì¸¡ê¸°ê¸°, ì œì–´ë¡œì§ ë“±)
3. ì•ˆì „ ê³ ë ¤ì‚¬í•­ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
4. ì¶”ê°€ ê¶Œì¥ì‚¬í•­ (í•„ìš”í•œ ê²½ìš°)"""

    def save_state(self, chunks, embeddings):
        """ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ì €ì¥"""
        os.makedirs(self.STATE_PATH, exist_ok=True)
        try:
            with open(os.path.join(self.STATE_PATH, "state.pkl"), "wb") as f:
                pickle.dump({"chunks": chunks, "embeddings": embeddings}, f)
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_state(self):
        """ì €ì¥ëœ ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ë¡œë“œ"""
        state_file = os.path.join(self.STATE_PATH, "state.pkl")
        if os.path.exists(state_file):
            try:
                with open(state_file, "rb") as f:
                    data = pickle.load(f)
                    return data["chunks"], data["embeddings"]
            except Exception as e:
                logger.error(f"ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

    def create_embeddings(self, chunks):
        """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ì—¬ í…ì„œë¡œ ë³€í™˜"""
        embeddings = self.embedder.encode(chunks, convert_to_tensor=True)
        return embeddings

    def initialize_rag_system(self, pdf_path: str) -> bool:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹œë„
            chunks, embeddings = self.load_state()
            
            if chunks is None or embeddings is None:
                logger.info("ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
                
                # PDF ë¡œë“œ ë° ì²­í‚¹
                if not os.path.exists(pdf_path):
                    logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                    return False
                
                documents = self.kiwi_rag.extract_text_from_pdf(pdf_path)
                
                if not documents:
                    logger.error("PDF ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨")
                    return False
                
                # ì²­í‚¹
                chunks_metadata = self.kiwi_rag.chunk_documents_with_kiwi(documents)
                chunks = [chunk['content'] for chunk in chunks_metadata]
                
                # ì„ë² ë”© ìƒì„±
                logger.info("ì„ë² ë”© ìƒì„± ì¤‘...")
                embeddings = self.create_embeddings(chunks)
                
                # ìƒíƒœ ì €ì¥
                self.save_state(chunks, embeddings)
            else:
                logger.info("ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            
            self.chunks = chunks
            self.embeddings = embeddings
            
            logger.info(f"RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {embeddings.shape} ì„ë² ë”©")
            return True
            
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def retrieve_relevant_chunks(self, query, top_k=3):
        """RAG ê²€ìƒ‰ - ê´€ë ¨ ì²­í¬ ì¶”ì¶œ"""
        if self.chunks is None or self.embeddings is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedder.encode(query, convert_to_tensor=True)
            query_embedding = query_embedding / torch.norm(query_embedding)
            
            # ì„ë² ë”© ì •ê·œí™”
            embeddings_norm = self.embeddings / torch.norm(self.embeddings, dim=1, keepdim=True)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.matmul(embeddings_norm, query_embedding)
            
            # ìƒìœ„ kê°œ ì¶”ì¶œ
            top_k_indices = torch.topk(similarities, min(top_k, len(similarities))).indices.tolist()
            top_k_scores = torch.topk(similarities, min(top_k, len(similarities))).values.tolist()
            
            # ê´€ë ¨ ì²­í¬ì™€ ì ìˆ˜ ë°˜í™˜
            relevant_chunks = []
            for i, (idx, score) in enumerate(zip(top_k_indices, top_k_scores)):
                relevant_chunks.append({
                    'content': self.chunks[idx],
                    'score': score,
                    'rank': i + 1,
                    'page': i + 1  # Streamlit í˜¸í™˜ì„±ì„ ìœ„í•´ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                })
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def build_rag_context(self, relevant_chunks):
        """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        context = ""
        for chunk in relevant_chunks:
            context += f"[ì°¸ê³ ìë£Œ {chunk['rank']}] (ìœ ì‚¬ë„: {chunk['score']:.3f})\n"
            context += f"{chunk['content']}\n\n"
        return context.strip()

    def create_pid_expert_prompt(self, user_question, rag_context):
        """P&ID ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        system_prompt = f"""{self.expert_persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ P&ID ì§€ì‹ì„ í™œìš©í•˜ë˜, ì¶”ì¸¡ì´ ì•„ë‹Œ í™•ì‹¤í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”."""

        return system_prompt

    def _detect_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ ìœ í˜• ê°ì§€ - ë³€ê²½ ë¶„ì„ë§Œ ì§€ì›"""
        
        # ë³€ê²½/ë¹„êµ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        change_keywords = [
            'ë³€ê²½', 'ë¹„êµ', 'ì°¨ì´', 'compare', 'difference', 'change',
            'as-is', 'to-be', 'asis', 'tobe', 'ì´ì „', 'ì´í›„', 'ì „í›„',
            'ìˆ˜ì •', 'ê°œì„ ', 'ì—…ë°ì´íŠ¸', 'ë°”ë€', 'ë‹¬ë¼ì§„'
        ]
        
        # ë¹„êµ í‘œí˜„ íŒ¨í„´
        comparison_patterns = [
            r'vs|versus',  # A vs B
            r'ì™€\s*ë¹„êµ',  # Aì™€ ë¹„êµ
            r'ê³¼\s*ë¹„êµ',  # Aê³¼ ë¹„êµ
            r'ëŒ€ë¹„',       # A ëŒ€ë¹„ B
            r'ì—ì„œ\s*.*ë¡œ',  # Aì—ì„œ Bë¡œ
            r'ê¸°ì¡´.*ìƒˆë¡œìš´',  # ê¸°ì¡´ A ìƒˆë¡œìš´ B
            r'ì´ì „.*í˜„ì¬',   # ì´ì „ A í˜„ì¬ B
        ]
        
        safety_keywords = ['ì•ˆì „', 'ìœ„í—˜', 'ë¹„ìƒ', 'ì •ì§€', 'ë³´í˜¸', 'ì•ŒëŒ', 'ESD', 'SIS', 'ì¸í„°ë¡']
        instrument_keywords = ['FT', 'FC', 'FV', 'PT', 'PC', 'TT', 'TC', 'LT', 'LC', 'AT', 'AC', 'ê³„ì¸¡ê¸°', 'íŠ¸ëœìŠ¤ë¯¸í„°', 'ì¡°ì ˆê¸°']
        
        query_lower = query.lower()
        
        # ë³€ê²½/ë¹„êµ í‚¤ì›Œë“œ í™•ì¸ (ìµœìš°ì„ )
        if any(keyword in query_lower for keyword in change_keywords):
            return "change_analysis"
        
        # ë¹„êµ íŒ¨í„´ í™•ì¸
        import re
        for pattern in comparison_patterns:
            if re.search(pattern, query):
                return "change_analysis"
        
        # ê¸°íƒ€ ë¶„ë¥˜
        if any(keyword in query for keyword in safety_keywords):
            return "safety_analysis"
        elif any(keyword in query for keyword in instrument_keywords):
            return "instrument_explanation"
        else:
            return "general"

    def create_change_analysis_prompt(self, user_question, rag_context):
        """ë³€ê²½ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - ë°ì´í„°ë² ì´ìŠ¤ ë„ë©´ ë°ì´í„° í¬í•¨"""
        
        change_expert_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ë³€ê²½ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- P&ID ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ë° ê²€í† 
- ê³µì • ê°œì„  ë° ì„¤ë¹„ êµì²´ ì˜í–¥ ë¶„ì„
- ë³€ê²½ ì „í›„ ë¹„êµ ë¶„ì„
- ë³€ê²½ì‚¬í•­ì˜ ì•ˆì „ì„± ë° ìš´ì „ì„± í‰ê°€

**ë³€ê²½ ë¶„ì„ ì ‘ê·¼ë²•:**
1. **ë³€ê²½ ì‚¬í•­ ì‹ë³„**: ì •í™•íˆ ë¬´ì—‡ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ íŒŒì•…
2. **ì˜í–¥ë„ ë¶„ì„**: ë³€ê²½ì´ ì£¼ë³€ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. **ì•ˆì „ì„± ê²€í† **: ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì•ˆì „ ë¦¬ìŠ¤í¬ í‰ê°€
4. **ìš´ì „ì„± ê²€í† **: ìš´ì „ ì ˆì°¨ ë° ìœ ì§€ë³´ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
5. **ê¶Œì¥ì‚¬í•­**: ë³€ê²½ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì¶”ê°€ ì‚¬í•­

**ë‹µë³€ êµ¬ì¡°:**
1. **ë³€ê²½ì‚¬í•­ ìš”ì•½** (ë¬´ì—‡ì´ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€)
2. **ê¸°ìˆ ì  ì˜í–¥ ë¶„ì„** (ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥)
3. **ì•ˆì „ì„± ê²€í† ** (ì•ˆì „ ê´€ë ¨ ê³ ë ¤ì‚¬í•­)
4. **ìš´ì „ ë° ìœ ì§€ë³´ìˆ˜ ì˜í–¥** (ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­)
5. **ê¶Œì¥ì‚¬í•­ ë° ì£¼ì˜ì‚¬í•­** (ì¶”ê°€ ê²€í†  í•„ìš” ì‚¬í•­)"""

        # ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
        drawing_context = ""
        
        try:
            # ì§ˆë¬¸ì—ì„œ íŒŒì¼ëª… íŒ¨í„´ ì°¾ê¸°
            import re
            
            # íŒŒì¼ëª… íŒ¨í„´ë“¤ (í™•ì¥ì í¬í•¨)
            file_patterns = [
                r'([a-zA-Z0-9ê°€-í£_\-\.]+\.(?:pdf|png|jpg|jpeg))',  # í™•ì¥ì í¬í•¨
                r'([a-zA-Z0-9ê°€-í£_\-\.]+)\s*(?:íŒŒì¼|ë„ë©´|ë¬¸ì„œ)',    # íŒŒì¼/ë„ë©´/ë¬¸ì„œ í‚¤ì›Œë“œ
                r'"([^"]+)"',  # ë”°ì˜´í‘œë¡œ ê°ì‹¼ íŒŒì¼ëª…
                r"'([^']+)'"   # ì‘ì€ë”°ì˜´í‘œë¡œ ê°ì‹¼ íŒŒì¼ëª…
            ]
            
            detected_filename = None
            for pattern in file_patterns:
                matches = re.findall(pattern, user_question, re.IGNORECASE)
                if matches:
                    detected_filename = matches[0]
                    break
            
            if detected_filename:
                logger.info(f"ì§ˆë¬¸ì—ì„œ íŒŒì¼ëª… ê°ì§€: {detected_filename}")
                
                # ë³€ê²½ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                change_keywords = ['ë³€ê²½', 'ìˆ˜ì •', 'ê°œì„ ', 'êµì²´', 'ì—…ê·¸ë ˆì´ë“œ', 'ì¡°ì •', 'ë¹„êµ', 'ì°¨ì´', 'ì „í›„']
                version_keywords = {
                    'latest': ['ìµœì‹ ', 'ìƒˆë¡œìš´', 'í˜„ì¬', 'ì—…ë°ì´íŠ¸ëœ', 'ì‹ ê·œ'],
                    'previous': ['ì´ì „', 'ê³¼ê±°', 'ì›ë˜', 'ê¸°ì¡´', 'ì˜›ë‚ ']
                }
                
                is_change_analysis = any(keyword in user_question for keyword in change_keywords)
                
                if is_change_analysis:
                    # ìµœì‹  ë²„ì „ê³¼ ì´ì „ ë²„ì „ ëª¨ë‘ ì¡°íšŒ
                    latest_data = self.get_drawing_data_from_db(detected_filename, "latest")
                    previous_data = self.get_drawing_data_from_db(detected_filename, "previous")
                    
                    if latest_data or previous_data:
                        drawing_context = "\n\n=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒëœ ë„ë©´ ì •ë³´ ===\n"
                        
                        if latest_data:
                            drawing_context += self.build_drawing_context(latest_data, "ìµœì‹ ")
                            drawing_context += "\n\n"
                        
                        if previous_data:
                            drawing_context += self.build_drawing_context(previous_data, "ì´ì „")
                            drawing_context += "\n\n"
                        
                        # ë¹„êµ ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
                        if latest_data and previous_data:
                            drawing_context += "=== ë²„ì „ ë¹„êµ ì •ë³´ ===\n"
                            drawing_context += f"ìµœì‹  ë²„ì „ ë“±ë¡ì¼: {latest_data.get('create_date')}\n"
                            drawing_context += f"ì´ì „ ë²„ì „ ë“±ë¡ì¼: {previous_data.get('create_date')}\n"
                            
                            # í…ìŠ¤íŠ¸ ë³€ê²½ ë¶„ì„
                            latest_text = self.extract_text_from_drawing_data(latest_data)
                            previous_text = self.extract_text_from_drawing_data(previous_data)
                            
                            if latest_text != previous_text:
                                drawing_context += "âš ï¸ ë„ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë³€ê²½ì‚¬í•­ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                            else:
                                drawing_context += "â„¹ï¸ ë„ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ì—ëŠ” ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.\n"
                    else:
                        drawing_context += f"\n\nâš ï¸ '{detected_filename}' íŒŒì¼ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                
                else:
                    # ë³€ê²½ ë¶„ì„ì´ ì•„ë‹Œ ê²½ìš° íŠ¹ì • ë²„ì „ ìš”ì²­ í™•ì¸
                    requested_version = "latest"  # ê¸°ë³¸ê°’
                    
                    for version, keywords in version_keywords.items():
                        if any(keyword in user_question for keyword in keywords):
                            requested_version = version
                            break
                    
                    drawing_data = self.get_drawing_data_from_db(detected_filename, requested_version)
                    
                    if drawing_data:
                        version_label = "ìµœì‹ " if requested_version == "latest" else "ì´ì „"
                        drawing_context = "\n\n=== ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒëœ ë„ë©´ ì •ë³´ ===\n"
                        drawing_context += self.build_drawing_context(drawing_data, version_label)
                    else:
                        drawing_context += f"\n\nâš ï¸ '{detected_filename}' ({requested_version})ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            drawing_context = "\n\nâš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"

        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""{change_expert_persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

**ë„ë©´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´:**
{drawing_context}

ì‚¬ìš©ìê°€ P&ID ë³€ê²½ ë˜ëŠ” ë¹„êµì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ë³€ê²½ ê´€ë¦¬ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

- ë³€ê²½ì‚¬í•­ì˜ êµ¬ì²´ì  ë‚´ìš©ê³¼ ë²”ìœ„
- ì—°ê´€ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
- ì•ˆì „ì„± ë° ìš´ì „ì„± ê´€ì ì—ì„œì˜ ê²€í† 
- ë³€ê²½ ì‹œ ì¶”ê°€ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­

ìœ„ ì°¸ê³  ë¬¸ì„œì™€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒëœ ë„ë©´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ì²´ê³„ì ì¸ ë³€ê²½ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

íŒŒì¼ëª…ì´ ê°ì§€ë˜ì—ˆë‹¤ë©´ í•´ë‹¹ ë„ë©´ì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."""

        return system_prompt

    def retrieve_change_analysis_chunks(self, query, top_k=5):
        """ë³€ê²½ ë¶„ì„ì„ ìœ„í•œ í™•ì¥ëœ ê²€ìƒ‰"""
        if self.chunks is None or self.embeddings is None:
            return []
        
        try:
            # ê¸°ë³¸ ê²€ìƒ‰
            basic_chunks = self.retrieve_relevant_chunks(query, top_k=3)
            
            # ë³€ê²½ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
            change_terms = ['ë³€ê²½', 'ìˆ˜ì •', 'ê°œì„ ', 'êµì²´', 'ì—…ê·¸ë ˆì´ë“œ', 'ì¡°ì •']
            additional_chunks = []
            
            for term in change_terms:
                if term not in query:  # ì´ë¯¸ í¬í•¨ëœ ìš©ì–´ëŠ” ì œì™¸
                    expanded_query = f"{query} {term}"
                    extra_chunks = self.retrieve_relevant_chunks(expanded_query, top_k=2)
                    additional_chunks.extend(extra_chunks)
            
            # ì¤‘ë³µ ì œê±° ë° í†µí•©
            all_chunks = basic_chunks + additional_chunks
            seen_contents = set()
            unique_chunks = []
            
            for chunk in all_chunks:
                if chunk['content'] not in seen_contents:
                    seen_contents.add(chunk['content'])
                    unique_chunks.append(chunk)
            
            # ìƒìœ„ top_kê°œ ë°˜í™˜
            return unique_chunks[:top_k]
            
        except Exception as e:
            logger.error(f"ë³€ê²½ ë¶„ì„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self.retrieve_relevant_chunks(query, top_k)

    def get_drawing_data_from_db(self, d_name: str, version: str = "latest") -> Optional[Dict]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë„ë©´ ë°ì´í„°ë¥¼ ì¡°íšŒ
        
        Args:
            d_name: ë„ë©´ íŒŒì¼ëª…
            version: "latest" (ìµœì‹ ) ë˜ëŠ” "previous" (ì´ì „)
        
        Returns:
            ë„ë©´ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return None
            
            cursor = conn.cursor()
            
            if version == "latest":
                # ìµœì‹  íŒŒì¼ (create_dateê°€ ê°€ì¥ ëŠ¦ì€ ê²ƒ)
                query = """
                SELECT d_id, d_name, "user", create_date, json_data, image_path
                FROM domyun 
                WHERE d_name = %s 
                ORDER BY create_date DESC 
                LIMIT 1
                """
            else:  # previous
                # ì´ì „ íŒŒì¼ (create_dateê°€ ë‘ ë²ˆì§¸ë¡œ ëŠ¦ì€ ê²ƒ)
                query = """
                SELECT d_id, d_name, "user", create_date, json_data, image_path
                FROM domyun 
                WHERE d_name = %s 
                ORDER BY create_date DESC 
                LIMIT 1 OFFSET 1
                """
            
            cursor.execute(query, (d_name,))
            result = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            if result:
                d_id, d_name, user, create_date, json_data, image_path = result
                return {
                    'd_id': d_id,
                    'd_name': d_name,
                    'user': user,
                    'create_date': create_date,
                    'json_data': json_data,
                    'image_path': image_path
                }
            else:
                logger.warning(f"ë„ë©´ '{d_name}' ({version})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def get_all_versions_of_drawing(self, d_name: str) -> List[Dict]:
        """
        íŠ¹ì • ë„ë©´ì˜ ëª¨ë“  ë²„ì „ì„ ì¡°íšŒ
        
        Args:
            d_name: ë„ë©´ íŒŒì¼ëª…
        
        Returns:
            ëª¨ë“  ë²„ì „ì˜ ë„ë©´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return []
            
            cursor = conn.cursor()
            
            query = """
            SELECT d_id, d_name, "user", create_date, json_data, image_path
            FROM domyun 
            WHERE d_name = %s 
            ORDER BY create_date DESC
            """
            
            cursor.execute(query, (d_name,))
            results = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            versions = []
            for result in results:
                d_id, d_name, user, create_date, json_data, image_path = result
                versions.append({
                    'd_id': d_id,
                    'd_name': d_name,
                    'user': user,
                    'create_date': create_date,
                    'json_data': json_data,
                    'image_path': image_path
                })
            
            return versions
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def extract_text_from_drawing_data(self, drawing_data: Dict) -> str:
        """
        ë„ë©´ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        
        Args:
            drawing_data: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¨ ë„ë©´ ë°ì´í„°
        
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        if not drawing_data or not drawing_data.get('json_data'):
            return ""
        
        try:
            json_data = drawing_data['json_data']
            extracted_texts = []
            
            # OCR ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if 'ocr_data' in json_data and json_data['ocr_data']:
                ocr_data = json_data['ocr_data']
                if 'images' in ocr_data:
                    for image in ocr_data['images']:
                        if 'fields' in image:
                            for field in image['fields']:
                                if 'inferText' in field:
                                    extracted_texts.append(field['inferText'])
            
            # Detection ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìˆë‹¤ë©´)
            if 'detection_data' in json_data and json_data['detection_data']:
                detection_data = json_data['detection_data']
                if 'detections' in detection_data:
                    for detection in detection_data['detections']:
                        if 'text' in detection:
                            extracted_texts.append(detection['text'])
            
            return '\n'.join(extracted_texts)
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""

    def build_drawing_context(self, drawing_data: Dict, version_label: str = "") -> str:
        """
        ë„ë©´ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ êµ¬ì„±
        
        Args:
            drawing_data: ë„ë©´ ë°ì´í„°
            version_label: ë²„ì „ ë¼ë²¨ (ì˜ˆ: "ìµœì‹ ", "ì´ì „")
        
        Returns:
            êµ¬ì„±ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not drawing_data:
            return ""
        
        context_parts = []
        
        # ë„ë©´ ê¸°ë³¸ ì •ë³´
        context_parts.append(f"=== {version_label} ë„ë©´ ì •ë³´ ===")
        context_parts.append(f"íŒŒì¼ëª…: {drawing_data.get('d_name', 'N/A')}")
        context_parts.append(f"ë“±ë¡ì¼: {drawing_data.get('create_date', 'N/A')}")
        context_parts.append(f"ë“±ë¡ì: {drawing_data.get('user', 'N/A')}")
        
        # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        extracted_text = self.extract_text_from_drawing_data(drawing_data)
        if extracted_text:
            context_parts.append(f"\n--- {version_label} ë„ë©´ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ---")
            context_parts.append(extracted_text)
        
        # JSON ë°ì´í„° ìš”ì•½
        json_data = drawing_data.get('json_data')
        if json_data:
            context_parts.append(f"\n--- {version_label} ë„ë©´ ë©”íƒ€ë°ì´í„° ---")
            
            # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
            if 'width' in json_data and 'height' in json_data:
                context_parts.append(f"ì´ë¯¸ì§€ í¬ê¸°: {json_data['width']} x {json_data['height']}")
            
            # OCR í†µê³„
            if 'ocr_data' in json_data and json_data['ocr_data']:
                ocr_data = json_data['ocr_data']
                if 'images' in ocr_data:
                    text_count = 0
                    for image in ocr_data['images']:
                        if 'fields' in image:
                            text_count += len(image['fields'])
                    context_parts.append(f"OCR ì¶”ì¶œ í…ìŠ¤íŠ¸ ê°œìˆ˜: {text_count}ê°œ")
            
            # Detection í†µê³„
            if 'detection_data' in json_data and json_data['detection_data']:
                detection_data = json_data['detection_data']
                if 'detections' in detection_data:
                    detection_count = len(detection_data['detections'])
                    context_parts.append(f"ê°ì§€ëœ ê°ì²´ ê°œìˆ˜: {detection_count}ê°œ")
        
        return '\n'.join(context_parts)

    def generate_response(self, user_query: str, use_web_search: bool = False, selected_drawing: str = None, selected_files: List[Dict] = None) -> Dict:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            use_web_search: ì›¹ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            selected_drawing: ì„ íƒëœ ë„ë©´ íŒŒì¼ëª…
            selected_files: ì„ íƒëœ íŒŒì¼ ëª©ë¡
            
        Returns:
            ì‘ë‹µ ë°ì´í„°
        """
        selected_drawing = '../uploads/uploaded_images/stream_dose_ai_1.png'
        try:
            # ì‹œê°í™” ìš”ì²­ ê°ì§€
            if "ì‹œê°í™”" in user_query and selected_drawing:
                # ì‹œê°í™” ìˆ˜í–‰
                print( 'ì‹œê°í™” ìˆ˜í–‰')
                viz_result = self.visualize_drawing_analysis(selected_drawing)
                print(viz_result)
                if not viz_result:
                    return {
                        'response': f"âŒ '{selected_drawing}' ë„ë©´ì˜ ì‹œê°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        'sources': [],
                        'query_type': 'drawing_visualization',
                        'context_quality': 'none',
                        'web_search_used': False,
                        'visualization': None
                    }
                
                return {
                    'response': viz_result['analysis_summary'],
                    'sources': [{
                        'type': 'visualization',
                        'icon': 'ğŸ¨',
                        'source': f'ë„ë©´ ì‹œê°í™” - {selected_drawing}',
                        'score': None,
                        'page': None,
                        'content_preview': f"OCR {viz_result['ocr_count']}ê°œ, Detection {viz_result['detection_count']}ê°œ ì‹œê°í™”",
                        'quality': 'high'
                    }],
                    'query_type': 'drawing_visualization',
                    'context_quality': 'high',
                    'web_search_used': False,
                    'visualization': viz_result
                }

            # ì„ íƒëœ íŒŒì¼ë“¤ ì²˜ë¦¬ ë° ë””ë²„ê·¸ ì •ë³´ ìˆ˜ì§‘
            selected_files_context = ""
            file_details = []
            ocr_data_included = False
            detection_data_included = False
            total_context_length = 0
            
            if selected_files and len(selected_files) > 0:
                logger.info(f"ğŸ“ ì„ íƒëœ íŒŒì¼ {len(selected_files)}ê°œ ì²˜ë¦¬ ì¤‘...")
                
                selected_files_context = "\n\n=== ì„ íƒëœ P&ID ë„ë©´ ê¸°í˜¸ ë° í…ìŠ¤íŠ¸ íƒì§€ ê²°ê³¼ ===\n"
                selected_files_context += "â€» ë‹¤ìŒ ë°ì´í„°ëŠ” P&ID ë„ë©´ì—ì„œ AIê°€ ìë™ìœ¼ë¡œ íƒì§€í•œ ê³„ì¸¡ê¸°ê¸° ê¸°í˜¸, ë°°ê´€ ê¸°í˜¸, í…ìŠ¤íŠ¸ ë¼ë²¨ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\n\n"
                
                for i, file_data in enumerate(selected_files):
                    file_name = file_data.get('name', f'íŒŒì¼_{i+1}')
                    file_id = file_data.get('id', 'unknown')
                    image_path = file_data.get('image_path')
                    json_data = file_data.get('json_data')
                    
                    selected_files_context += f"\n**ğŸ“‹ P&ID ë„ë©´ {i+1}: {file_name} (ID: {file_id})**\n"
                    
                    # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ ì´ˆê¸°í™”
                    file_detail = {
                        'name': file_name,
                        'id': file_id,
                        'ocr_count': 0,
                        'detection_count': 0,
                        'json_size': 0,
                        'ocr_preview': '',
                        'detection_preview': ''
                    }
                    
                    # ì´ë¯¸ì§€ ì •ë³´
                    if image_path and os.path.exists(image_path):
                        selected_files_context += f"- ğŸ“· ë„ë©´ ì´ë¯¸ì§€ ê²½ë¡œ: {image_path}\n"
                    else:
                        selected_files_context += f"- ğŸ“· ë„ë©´ ì´ë¯¸ì§€: ì—†ìŒ\n"
                    
                    # JSON ë°ì´í„° ìƒì„¸ ì²˜ë¦¬
                    if json_data:
                        file_detail['json_size'] = len(str(json_data))
                        
                        # OCR í…ìŠ¤íŠ¸ ì™„ì „ ì¶”ì¶œ ë° ìƒì„¸ í¬í•¨
                        ocr_texts = self._extract_ocr_texts(json_data)
                        if ocr_texts:
                            ocr_data_included = True
                            file_detail['ocr_count'] = len(ocr_texts)
                            file_detail['ocr_preview'] = ', '.join(ocr_texts[:10])
                            
                            selected_files_context += f"- ğŸ“ **OCR íƒì§€ í…ìŠ¤íŠ¸** (ê³„ì¸¡ê¸° íƒœê·¸ëª…, ë¼ë²¨, ì„¤ë¹„ëª… ë“± {len(ocr_texts)}ê°œ):\n"
                            for j, text in enumerate(ocr_texts):
                                selected_files_context += f"  {j+1}. \"{text}\"\n"
                            selected_files_context += "\n"
                        
                        # Detection ì •ë³´ ì™„ì „ ì¶”ì¶œ ë° ìƒì„¸ í¬í•¨
                        detection_info = self._extract_detection_info(json_data)
                        if detection_info:
                            detection_data_included = True
                            file_detail['detection_count'] = len(detection_info)
                            labels = [d.get('label', 'Unknown') for d in detection_info]
                            file_detail['detection_preview'] = ', '.join(labels[:10])
                            
                            selected_files_context += f"- ğŸ¯ **ê°ì²´ íƒì§€ ê²°ê³¼** (P&ID ê¸°í˜¸, ê³„ì¸¡ê¸°ê¸°, ë°¸ë¸Œ, ë°°ê´€ ë“± {len(detection_info)}ê°œ):\n"
                            for j, obj in enumerate(detection_info):
                                label = obj.get('label', obj.get('id', f'ê°ì²´{j+1}'))
                                
                                # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                                if 'boundingBox' in obj:
                                    bbox = obj['boundingBox']
                                    pos = f"({bbox.get('x', 0):.1f}, {bbox.get('y', 0):.1f})"
                                elif all(k in obj for k in ['x', 'y']):
                                    pos = f"({obj['x']}, {obj['y']})"
                                else:
                                    pos = "ìœ„ì¹˜ì •ë³´ì—†ìŒ"
                                
                                selected_files_context += f"  {j+1}. ğŸ”§ {label} (ë„ë©´ì¢Œí‘œ: {pos})\n"
                            selected_files_context += "\n"
                        
                        # JSON ì›ì‹œ ë°ì´í„° êµ¬ì¡° ì •ë³´ ì¶”ê°€
                        selected_files_context += f"- ğŸ“Š **AI íƒì§€ ë°ì´í„° êµ¬ì¡°:**\n"
                        if isinstance(json_data, dict):
                            for key in json_data.keys():
                                if key == 'ocr' or key == 'ocr_data':
                                    selected_files_context += f"  â€¢ {key} (ë¬¸ì ì¸ì‹ ë°ì´í„°)\n"
                                elif key == 'detecting' or key == 'detection_data':
                                    selected_files_context += f"  â€¢ {key} (ê¸°í˜¸/ê°ì²´ íƒì§€ ë°ì´í„°)\n"
                                else:
                                    selected_files_context += f"  â€¢ {key}\n"
                        selected_files_context += "\n"
                        
                    else:
                        selected_files_context += f"- ğŸ“Š AI íƒì§€ ë°ì´í„°: ì—†ìŒ\n\n"
                    
                    file_details.append(file_detail)
                
                total_context_length = len(selected_files_context)
                logger.info(f"âœ… P&ID ë„ë©´ íƒì§€ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(selected_files)}ê°œ íŒŒì¼")

            # ì¿¼ë¦¬ ìœ í˜• ê°ì§€
            query_type = self._detect_query_type(user_query)
            
            # ë³€ê²½ ë¶„ì„ ì²˜ë¦¬ (ìµœìš°ì„ )
            if query_type == "change_analysis":
                logger.info(f"ğŸ”„ ë³€ê²½ ë¶„ì„ ëª¨ë“œë¡œ ì²˜ë¦¬: {user_query}")
                
                # ì§ì ‘ ë³€ê²½ ë¶„ì„ ìˆ˜í–‰ (ë„ë©´ ì´ë¦„ ìƒê´€ì—†ì´ stream_dose_ai_1ê³¼ stream_dose_ai_3 ë¹„êµ)
                change_result = self.analyze_drawing_changes(user_query)
                
                # ë³€ê²½ ë¶„ì„ì´ ì„±ê³µí•œ ê²½ìš° ë°”ë¡œ ë°˜í™˜
                if change_result and change_result.get('visualization'):
                    return change_result
                else:
                    logger.warning("âš ï¸ ë³€ê²½ ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    return {
                        'response': "ë³€ê²½ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. stream_dose_ai_1.jsonê³¼ stream_dose_ai_3.json íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                        'sources': [],
                        'query_type': 'change_analysis',
                        'context_quality': 'none',
                        'web_search_used': False,
                        'visualization': None
                    }
            
            # RAG ê²€ìƒ‰ ìˆ˜í–‰ (ì¼ë°˜ ì§ˆë¬¸ì˜ ê²½ìš°)
            relevant_chunks = self.retrieve_relevant_chunks(user_query, top_k=3)
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ì†ŒìŠ¤ ì„ íƒ ë¡œì§
            SIMILARITY_THRESHOLD = 0.4
            high_quality_chunks = []
            low_quality_chunks = []
            
            for chunk in relevant_chunks:
                if chunk['score'] >= SIMILARITY_THRESHOLD:
                    high_quality_chunks.append(chunk)
                else:
                    low_quality_chunks.append(chunk)
            
            # ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
            sources = []
            rag_context = ""
            web_search_used = False
            web_search_results = ""
            
            # ê³ í’ˆì§ˆ RAG ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            if high_quality_chunks:
                logger.info(f"ğŸ“– ê³ í’ˆì§ˆ RAG ë°ì´í„° {len(high_quality_chunks)}ê°œ ë°œê²¬ (ìœ ì‚¬ë„ â‰¥ {SIMILARITY_THRESHOLD})")
                
                rag_context = self.build_rag_context(high_quality_chunks)
                
                # RAG ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                for chunk in high_quality_chunks:
                    sources.append({
                        'type': 'rag',
                        'icon': 'ğŸ“–',
                        'source': 'RAG ë°ì´í„°ë² ì´ìŠ¤',
                        'score': chunk['score'],
                        'page': chunk['page'],
                        'content_preview': chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content'],
                        'quality': 'high'
                    })
            
            # ì„ íƒëœ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ì†ŒìŠ¤ì— ì¶”ê°€
            if selected_files:
                for file_data in selected_files:
                    sources.append({
                        'type': 'file',
                        'icon': 'ğŸ“„',
                        'source': f"ì„ íƒëœ íŒŒì¼: {file_data.get('name', 'Unknown')}",
                        'score': None,
                        'page': None,
                        'content_preview': f"íŒŒì¼ ID: {file_data.get('id')}, ë“±ë¡ì¼: {file_data.get('create_date')}",
                        'quality': 'high'
                    })
            
            # OpenAI API í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
            messages = []
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒëœ íŒŒì¼ ë°ì´í„°ë¥¼ ìƒì„¸íˆ í¬í•¨)
            system_prompt = f"""{self.expert_persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

**P&ID ë„ë©´ AI íƒì§€ ë°ì´í„°:**
{selected_files_context}

ìœ„ ì°¸ê³  ë¬¸ì„œì™€ ì„ íƒëœ P&ID ë„ë©´ì˜ AI íƒì§€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 

**ì¤‘ìš”:** ì œê³µëœ íƒì§€ ë°ì´í„°ëŠ” P&ID ë„ë©´ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„ëœ ê²°ê³¼ì…ë‹ˆë‹¤:
- **OCR íƒì§€ í…ìŠ¤íŠ¸**: ë„ë©´ì—ì„œ ì¸ì‹ëœ ê³„ì¸¡ê¸° íƒœê·¸ëª…(FT-101, PT-201 ë“±), ì„¤ë¹„ëª…, ë¼ë²¨, ìˆ˜ì¹˜ ë“±
- **ê°ì²´ íƒì§€ ê²°ê³¼**: AIê°€ ì‹ë³„í•œ P&ID ê¸°í˜¸ë“¤ (ê³„ì¸¡ê¸°ê¸°, ë°¸ë¸Œ, íŒí”„, ë°°ê´€, ì œì–´ê¸°ê¸° ë“±)

ì´ëŸ¬í•œ êµ¬ì²´ì ì¸ P&ID ìš”ì†Œë“¤ì„ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë„ë©´ í•´ì„ê³¼ ê³µì • ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            messages.append({"role": "system", "content": system_prompt})
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ (í…ìŠ¤íŠ¸ë§Œ)
            messages.append({"role": "user", "content": user_query})
            
            # OpenAI API í˜¸ì¶œ
            if not self.client:
                return {
                    'response': "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    'sources': sources,
                    'query_type': query_type,
                    'context_quality': 'none',
                    'web_search_used': web_search_used,
                    'similarity_threshold': SIMILARITY_THRESHOLD,
                    'selected_drawing': selected_drawing,
                    'selected_files_count': len(selected_files) if selected_files else 0
                }
            
            try:
                # í•­ìƒ gpt-4o-mini ì‚¬ìš© (Vision API ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=0.3,
                    max_tokens=2000
                )
                
                ai_response = response.choices[0].message.content
                
                # ì‘ë‹µì— íŒŒì¼ ì •ë³´ ì¶”ê°€
                if selected_files:
                    file_info = f"\n\nğŸ“ **ë¶„ì„ëœ íŒŒì¼ ({len(selected_files)}ê°œ):**\n"
                    for file_data in selected_files:
                        file_info += f"â€¢ {file_data.get('name', 'Unknown')} (ID: {file_data.get('id')})\n"
                    ai_response += file_info
                
                # ì‘ë‹µì— ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ ì¶”ê°€
                source_info = self._build_source_summary(sources, SIMILARITY_THRESHOLD)
                if source_info:
                    ai_response += f"\n\n{source_info}"
                
            except Exception as e:
                logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                ai_response = f"OpenAI API ì˜¤ë¥˜: {e}"
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.conversation_history.append({
                'timestamp': datetime.now(),
                'user_query': user_query,
                'response': ai_response,
                'query_type': query_type,
                'context_quality': 'high' if high_quality_chunks or selected_files else 'medium',
                'sources_count': len(sources),
                'web_search_used': web_search_used,
                'similarity_threshold': SIMILARITY_THRESHOLD,
                'selected_drawing': selected_drawing,
                'selected_files_count': len(selected_files) if selected_files else 0,
                'images_processed': 0  # ì´ë¯¸ì§€ ì²˜ë¦¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0
            })
            
            return {
                'response': ai_response,
                'sources': sources,
                'query_type': query_type,
                'context_quality': 'high' if high_quality_chunks or selected_files else 'medium',
                'web_search_used': web_search_used,
                'similarity_threshold': SIMILARITY_THRESHOLD,
                'selected_drawing': selected_drawing,
                'selected_files_count': len(selected_files) if selected_files else 0,
                # ë””ë²„ê·¸ ì •ë³´ì— íŒŒì¼ ìƒì„¸ ì •ë³´ ì¶”ê°€
                'ocr_data_included': ocr_data_included,
                'detection_data_included': detection_data_included,
                'total_context_length': total_context_length,
                'file_details': file_details
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'response': f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'query_type': 'error',
                'context_quality': 'none',
                'web_search_used': False,
                'similarity_threshold': 0.4,
                'selected_drawing': selected_drawing,
                'selected_files_count': len(selected_files) if selected_files else 0
            }

    def _build_source_summary(self, sources: List[Dict], threshold: float) -> str:
        """ì†ŒìŠ¤ ìš”ì•½ ì •ë³´ ìƒì„±"""
        if not sources:
            return ""
        
        summary_parts = []
        summary_parts.append("---")
        summary_parts.append("**ğŸ“‹ ì •ë³´ ì¶œì²˜:**")
        
        rag_sources = [s for s in sources if s['type'] == 'rag']
        web_sources = [s for s in sources if s['type'] == 'web']
        
        if rag_sources:
            high_quality = [s for s in rag_sources if s.get('quality') == 'high']
            low_quality = [s for s in rag_sources if s.get('quality') == 'low']
            
            if high_quality:
                summary_parts.append(f"ğŸ“– **RAG ë°ì´í„°ë² ì´ìŠ¤** (ê³ í’ˆì§ˆ, ìœ ì‚¬ë„ â‰¥ {threshold}): {len(high_quality)}ê°œ")
                for source in high_quality:
                    summary_parts.append(f"  â€¢ í˜ì´ì§€ {source['page']}, ìœ ì‚¬ë„: {source['score']:.3f}")
            
            if low_quality:
                summary_parts.append(f"ğŸ“– **RAG ë°ì´í„°ë² ì´ìŠ¤** (ì°¸ê³ ìš©, ìœ ì‚¬ë„ < {threshold}): {len(low_quality)}ê°œ")
                for source in low_quality:
                    summary_parts.append(f"  â€¢ í˜ì´ì§€ {source['page']}, ìœ ì‚¬ë„: {source['score']:.3f}")
        
        if web_sources:
            summary_parts.append(f"ğŸŒ **ì¸í„°ë„· ê²€ìƒ‰**: {len(web_sources)}ê°œ")
            summary_parts.append("  â€¢ GPT-4 ê¸°ë°˜ ìµœì‹  ì •ë³´ ê²€ìƒ‰")
        
        return "\n".join(summary_parts)

    def get_conversation_summary(self) -> Dict:
        """ëŒ€í™” ìš”ì•½ í†µê³„"""
        if not self.conversation_history:
            return {}
        
        total_queries = len(self.conversation_history)
        query_types = {}
        context_qualities = {}
        
        for conv in self.conversation_history:
            query_type = conv.get('query_type', 'unknown')
            context_quality = conv.get('context_quality', 'unknown')
            
            query_types[query_type] = query_types.get(query_type, 0) + 1
            context_qualities[context_quality] = context_qualities.get(context_quality, 0) + 1
        
        return {
            'total_queries': total_queries,
            'query_types': query_types,
            'context_qualities': context_qualities,
            'average_sources_per_query': sum(conv.get('sources_count', 0) for conv in self.conversation_history) / total_queries
        }

    def clear_conversation_history(self):
        """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
        self.conversation_history = []
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def export_conversation_history(self) -> str:
        """ëŒ€í™” ê¸°ë¡ ë‚´ë³´ë‚´ê¸°"""
        if not self.conversation_history:
            return "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        export_text = "P&ID ì „ë¬¸ê°€ ì±—ë´‡ ëŒ€í™” ê¸°ë¡\n"
        export_text += "=" * 50 + "\n\n"
        
        for i, conv in enumerate(self.conversation_history, 1):
            export_text += f"ëŒ€í™” {i}\n"
            export_text += f"ì‹œê°„: {conv['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            export_text += f"ì§ˆë¬¸: {conv['user_query']}\n"
            export_text += f"ë‹µë³€: {conv['response']}\n"
            export_text += f"ì¿¼ë¦¬ ìœ í˜•: {conv['query_type']}\n"
            export_text += f"ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ: {conv['context_quality']}\n"
            export_text += "-" * 30 + "\n\n"
        
        return export_text

    def create_internal_data_prompt(self, user_question, rag_context):
        """ë‚´ë¶€ ë°ì´í„° ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± - ì›¹ ê²€ìƒ‰ ì—†ì´ RAGë§Œ ì‚¬ìš©"""
        
        internal_expert_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ë° ê³µì • ì œì–´ ì‹œìŠ¤í…œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- P&ID ë„ë©´ í•´ì„ ë° ë¶„ì„
- ê³µì • ì‹œìŠ¤í…œ ì„¤ê³„ ë° ìš´ì „
- ê³„ì¸¡ ë° ì œì–´ ì‹œìŠ¤í…œ ë¶„ì„
- ê³µì • ì•ˆì „ ê´€ë¦¬
- ì„¤ë¹„ ë° ì¥ì¹˜ ìš´ì „

**ì¤‘ìš” ì›ì¹™:**
âš ï¸ **ê¸°ë°€ì„± ì¤€ìˆ˜**: ì œê³µëœ ë‚´ë¶€ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ì™¸ë¶€ ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ğŸ“‹ **ì •í™•ì„± ìš°ì„ **: ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•Šìœ¼ë©°, ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤.
ğŸ” **ìƒì„¸ ë¶„ì„**: ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì²´ê³„ì ì´ê³  ìƒì„¸í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤.

**ë‹µë³€ êµ¬ì¡°:**
1. **í•µì‹¬ ìš”ì•½** (ë¬¸ì„œ ê¸°ë°˜ ì£¼ìš” ë‚´ìš©)
2. **ìƒì„¸ ë¶„ì„** (ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­)
3. **ìš´ì „ ê´€ë ¨ ì‚¬í•­** (ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­)
4. **ì£¼ì˜ì‚¬í•­** (ì•ˆì „ ë° ì œì•½ ì¡°ê±´)
5. **ë¬¸ì„œ ê¸°ë°˜ ì œí•œì‚¬í•­** (í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„)

**ë‹µë³€ ìŠ¤íƒ€ì¼:**
- ë¬¸ì„œì— ê·¼ê±°í•œ ì •í™•í•œ ì •ë³´ë§Œ ì œê³µ
- "ë¬¸ì„œì— ë”°ë¥´ë©´...", "ì œê³µëœ ìë£Œì—ì„œ..." ë“±ì˜ í‘œí˜„ ì‚¬ìš©
- ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ" ëª…ì‹œ"""

        # ë‚´ë¶€ ë°ì´í„° ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = f"""{internal_expert_persona}

**ì œê³µëœ ë‚´ë¶€ ë¬¸ì„œ:**
{rag_context if rag_context else "ê´€ë ¨ ë‚´ë¶€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}

**ì¤‘ìš” ì§€ì¹¨:**
1. ì˜¤ì§ ìœ„ì— ì œê³µëœ ë‚´ë¶€ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¼ë°˜ì ì¸ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
4. ëª¨ë“  ë‹µë³€ì€ ë¬¸ì„œì˜ êµ¬ì²´ì  ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”

ìœ„ ì§€ì¹¨ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        return system_prompt

    def get_all_drawing_names(self) -> List[str]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  ë„ë©´ íŒŒì¼ëª…ì„ ì¡°íšŒ
        
        Returns:
            ë„ë©´ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°)
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return []
            
            cursor = conn.cursor()
            
            query = """
            SELECT DISTINCT d_name
            FROM domyun 
            ORDER BY d_name
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
            drawing_names = [result[0] for result in results if result[0]]
            
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(drawing_names)}ê°œì˜ ë„ë©´ íŒŒì¼ëª… ì¡°íšŒë¨")
            return drawing_names
                
        except Exception as e:
            logger.error(f"ë„ë©´ íŒŒì¼ëª… ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def get_drawing_versions_info(self, d_name: str) -> List[Dict]:
        """
        íŠ¹ì • ë„ë©´ì˜ ëª¨ë“  ë²„ì „ ì •ë³´ë¥¼ ì¡°íšŒ (ë©”íƒ€ë°ì´í„°ë§Œ)
        
        Args:
            d_name: ë„ë©´ íŒŒì¼ëª…
        
        Returns:
            ë²„ì „ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return []
            
            cursor = conn.cursor()
            
            query = """
            SELECT d_id, "user", create_date
            FROM domyun 
            WHERE d_name = %s 
            ORDER BY create_date DESC
            """
            
            cursor.execute(query, (d_name,))
            results = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            versions = []
            for i, result in enumerate(results):
                d_id, user, create_date = result
                versions.append({
                    'd_id': d_id,
                    'user': user,
                    'create_date': create_date,
                    'version_label': f"ë²„ì „ {i+1}" if i > 0 else "ìµœì‹ ",
                    'is_latest': i == 0
                })
            
            return versions
                
        except Exception as e:
            logger.error(f"ë„ë©´ ë²„ì „ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def generate_drawing_summary(self, d_name: str, version: str = "latest") -> Dict:
        """
        íŠ¹ì • ë„ë©´ì˜ ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±
        
        Args:
            d_name: ë„ë©´ íŒŒì¼ëª…
            version: "latest" (ìµœì‹ ) ë˜ëŠ” "previous" (ì´ì „) ë˜ëŠ” d_id
        
        Returns:
            ìš”ì•½ ì •ë³´ê°€ í¬í•¨ëœ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ë„ë©´ ë°ì´í„° ì¡°íšŒ
            if version.isdigit():
                # d_idë¡œ ì§ì ‘ ì¡°íšŒ
                drawing_data = self.get_drawing_data_by_id(int(version))
            else:
                drawing_data = self.get_drawing_data_from_db(d_name, version)
            
            if not drawing_data:
                return {
                    'response': f"âŒ '{d_name}' ({version}) ë„ë©´ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    'sources': [],
                    'query_type': 'drawing_summary',
                    'context_quality': 'none',
                    'web_search_used': False,
                    'drawing_data': None
                }
            
            # ë„ë©´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = self.extract_text_from_drawing_data(drawing_data)
            
            # ìš”ì•½ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""ë‹¹ì‹ ì€ P&ID ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ P&ID ë„ë©´ì˜ AI íƒì§€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  êµ¬ì¡°í™”ëœ ë„ë©´ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ğŸ“‹ P&ID ë„ë©´ ì •ë³´:**
- íŒŒì¼ëª…: {drawing_data.get('d_name')}
- ë“±ë¡ì¼: {drawing_data.get('create_date')}
- ë“±ë¡ì: {drawing_data.get('user')}

**ğŸ” AIê°€ P&ID ë„ë©´ì—ì„œ íƒì§€í•œ í…ìŠ¤íŠ¸ (ê³„ì¸¡ê¸° íƒœê·¸ëª…, ì„¤ë¹„ëª…, ë¼ë²¨ ë“±):**
{extracted_text if extracted_text else 'AI í…ìŠ¤íŠ¸ íƒì§€ ê²°ê³¼ ì—†ìŒ - ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ì„ í•„ìš”'}

**ì¤‘ìš”:** ìœ„ í…ìŠ¤íŠ¸ëŠ” AIê°€ P&ID ë„ë©´ì—ì„œ ìë™ìœ¼ë¡œ ì¸ì‹í•œ ê³„ì¸¡ê¸° íƒœê·¸ëª…(FT-101, PT-201 ë“±), ì„¤ë¹„ëª…, ë°°ê´€ ë²ˆí˜¸, ì œì–´ ì •ë³´ ë“±ì…ë‹ˆë‹¤.

**ìš”ì•½ êµ¬ì¡°:**
1. **P&ID ë„ë©´ ê°œìš”** (ë„ë©´ì˜ ê³µì • ëª©ì ê³¼ ì£¼ìš” ê¸°ëŠ¥)
2. **ì£¼ìš” P&ID êµ¬ì„± ìš”ì†Œ** (íƒì§€ëœ ê³„ì¸¡ê¸°ê¸°, ë°¸ë¸Œ, íŒí”„, íƒ±í¬ ë“±)
3. **ì œì–´ ì‹œìŠ¤í…œ ë¶„ì„** (ì œì–´ ë£¨í”„, ì¸í„°ë¡, ì•ˆì „ì¥ì¹˜)
4. **ê³µì • ìš´ì „ íŠ¹ì„±** (ì£¼ìš” ìš´ì „ ì¡°ê±´ ë° ì ˆì°¨)
5. **ì•ˆì „ ì‹œìŠ¤í…œ ê²€í† ** (ì•ˆì „ì¥ì¹˜ ë° ë¹„ìƒëŒ€ì‘ ì‹œìŠ¤í…œ)

ê° ì„¹ì…˜ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ê³ , AIê°€ íƒì§€í•œ êµ¬ì²´ì ì¸ ê³„ì¸¡ê¸° íƒœê·¸ ë²ˆí˜¸ë‚˜ ì„¤ë¹„ëª…ì„ ì ê·¹ í™œìš©í•´ì£¼ì„¸ìš”.
íƒì§€ëœ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¼ë°˜ì ì¸ P&ID ë¶„ì„ ì›ì¹™ì— ë”°ë¼ ë³´ì™„ ì„¤ëª…ì„ ì œê³µí•˜ë˜, 
"AI íƒì§€ ì •ë³´ ê¸°ë°˜" vs "ì¼ë°˜ì  P&ID í•´ì„" ì„ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”."""

            # OpenAI API í˜¸ì¶œ
            if not self.client:
                return {
                    'response': "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    'sources': [],
                    'query_type': 'drawing_summary',
                    'context_quality': 'none',
                    'web_search_used': False,
                    'drawing_data': drawing_data
                }
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë„ë©´ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                        {"role": "user", "content": summary_prompt}
                    ],
                    temperature=0.2,
                    max_tokens=2000
                )
                
                summary_response = response.choices[0].message.content
                
                # ë„ë©´ ì •ë³´ ì¶”ê°€
                final_response = f"""ğŸ“‹ **ë„ë©´ ìš”ì•½ ë¶„ì„**

**ğŸ“„ ë„ë©´ ì •ë³´:**
- **íŒŒì¼ëª…:** {drawing_data.get('d_name')}
- **ë“±ë¡ì¼:** {drawing_data.get('create_date')}
- **ë“±ë¡ì:** {drawing_data.get('user')}
- **ë²„ì „:** {version}

---

{summary_response}

---

**ğŸ“Š ì¶”ì¶œ í†µê³„:**
- **í…ìŠ¤íŠ¸ ê¸¸ì´:** {len(extracted_text)} ë¬¸ì
- **JSON ë°ì´í„°:** {'ìˆìŒ' if drawing_data.get('json_data') else 'ì—†ìŒ'}
"""
                
                # ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
                sources = [{
                    'type': 'database',
                    'icon': 'ğŸ—„ï¸',
                    'source': f'ë„ë©´ ë°ì´í„°ë² ì´ìŠ¤ - {d_name}',
                    'score': None,
                    'page': None,
                    'content_preview': f"ë“±ë¡ì¼: {drawing_data.get('create_date')}, ë“±ë¡ì: {drawing_data.get('user')}",
                    'quality': 'high'
                }]
                
                return {
                    'response': final_response,
                    'sources': sources,
                    'query_type': 'drawing_summary',
                    'context_quality': 'high',
                    'web_search_used': False,
                    'drawing_data': drawing_data,
                    'extracted_text_length': len(extracted_text)
                }
                
            except Exception as e:
                logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                return {
                    'response': f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    'sources': [],
                    'query_type': 'drawing_summary',
                    'context_quality': 'none',
                    'web_search_used': False,
                    'drawing_data': drawing_data
                }
                
        except Exception as e:
            logger.error(f"ë„ë©´ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'response': f"ë„ë©´ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'query_type': 'drawing_summary',
                'context_quality': 'none',
                'web_search_used': False,
                'drawing_data': None
            }

    def get_drawing_data_by_id(self, d_id: int) -> Optional[Dict]:
        """
        d_idë¡œ íŠ¹ì • ë„ë©´ ë°ì´í„°ë¥¼ ì¡°íšŒ
        
        Args:
            d_id: ë„ë©´ ID
        
        Returns:
            ë„ë©´ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return None
            
            cursor = conn.cursor()
            
            query = """
            SELECT d_id, d_name, "user", create_date, json_data, image_path
            FROM domyun 
            WHERE d_id = %s
            """
            
            cursor.execute(query, (d_id,))
            result = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            if result:
                d_id, d_name, user, create_date, json_data, image_path = result
                return {
                    'd_id': d_id,
                    'd_name': d_name,
                    'user': user,
                    'create_date': create_date,
                    'json_data': json_data,
                    'image_path': image_path
                }
            else:
                logger.warning(f"ë„ë©´ ID '{d_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def search_drawings_by_name(self, search_term: str) -> List[Dict]:
        """
        ë„ë©´ ì´ë¦„ìœ¼ë¡œ LIKE ê²€ìƒ‰ì„ ìˆ˜í–‰
        
        Args:
            search_term: ê²€ìƒ‰í•  ë„ë©´ ì´ë¦„ (ë¶€ë¶„ ê²€ìƒ‰ ê°€ëŠ¥)
        
        Returns:
            ê²€ìƒ‰ëœ ë„ë©´ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return []
            
            cursor = conn.cursor()
            
            # LIKE ê²€ìƒ‰ ì¿¼ë¦¬ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            query = """
            SELECT DISTINCT d_name,
                   COUNT(*) as version_count,
                   MAX(create_date) as latest_date,
                   STRING_AGG(DISTINCT "user", ', ') as users
            FROM domyun 
            WHERE LOWER(d_name) LIKE LOWER(%s)
            GROUP BY d_name
            ORDER BY latest_date DESC
            """
            
            search_pattern = f"%{search_term}%"
            cursor.execute(query, (search_pattern,))
            results = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            drawings = []
            for d_name, version_count, latest_date, users in results:
                drawings.append({
                    'd_name': d_name,
                    'version_count': version_count,
                    'latest_date': latest_date,
                    'users': users
                })
            
            logger.info(f"ë„ë©´ ê²€ìƒ‰ '{search_term}': {len(drawings)}ê°œ ê²°ê³¼")
            return drawings
                
        except Exception as e:
            logger.error(f"ë„ë©´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def extract_drawing_names_from_query(self, query: str) -> List[str]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë„ë©´ ì´ë¦„ í›„ë³´ë¥¼ ì¶”ì¶œ
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
        
        Returns:
            ì¶”ì¶œëœ ë„ë©´ ì´ë¦„ í›„ë³´ ë¦¬ìŠ¤íŠ¸
        """
        import re
        
        # ë„ë©´ ì´ë¦„ íŒ¨í„´ë“¤
        drawing_patterns = [
            r'([a-zA-Z0-9_\-]+\.(?:pdf|png|jpg|jpeg))',  # í™•ì¥ì í¬í•¨ íŒŒì¼ëª…
            r'(stream_[a-zA-Z0-9_\-]+)',  # streamìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ëª…
            r'([a-zA-Z0-9_\-]{5,})',  # 5ìë¦¬ ì´ìƒ ì˜ìˆ«ì+ì–¸ë”ìŠ¤ì½”ì–´
        ]
        
        # ë„ë©´ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ì²˜ë¦¬
        drawing_keywords = ['ë„ë©´', 'íŒŒì¼', 'ê·¸ë¦¼', 'pdf', 'stream', 'does', 'ai']
        has_drawing_keyword = any(keyword.lower() in query.lower() for keyword in drawing_keywords)
        
        if not has_drawing_keyword:
            return []
        
        candidates = set()
        
        # ê° íŒ¨í„´ìœ¼ë¡œ í›„ë³´ ì¶”ì¶œ
        for pattern in drawing_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            candidates.update(matches)
        
        # ë”°ì˜´í‘œë‚˜ ê´„í˜¸ë¡œ ê°ì‹¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        quoted_patterns = [
            r'"([^"]+)"',  # í°ë”°ì˜´í‘œ
            r"'([^']+)'",  # ì‘ì€ë”°ì˜´í‘œ
            r'\(([^)]+)\)',  # ê´„í˜¸
        ]
        
        for pattern in quoted_patterns:
            matches = re.findall(pattern, query)
            candidates.update(matches)
        
        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ ì œì™¸
        exclude_words = {
            'ë„ë©´', 'íŒŒì¼', 'ê·¸ë¦¼', 'ì´ë¯¸ì§€', 'ë¶„ì„', 'ë¹„êµ', 'ë³€ê²½', 'ì°¨ì´',
            'ë„ë©´ì˜', 'íŒŒì¼ì˜', 'ê·¸ë¦¼ì˜', 'ì´ë¯¸ì§€ì˜', 'ê²ƒ', 'ê²ƒì˜', 'ë¶€ë¶„',
            'ë‚´ìš©', 'ì •ë³´', 'ë°ì´í„°', 'ê²°ê³¼', 'ì¶œë ¥', 'ì…ë ¥', 'ì²˜ë¦¬'
        }
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ í›„ë³´ í•„í„°ë§ ë° ì œì™¸ ë‹¨ì–´ í•„í„°ë§
        filtered_candidates = []
        for candidate in candidates:
            candidate = candidate.strip()
            if (3 <= len(candidate) <= 50 and 
                candidate.lower() not in exclude_words and
                not candidate.lower().endswith('ì˜') and
                not candidate.lower().startswith('ì˜')):
                filtered_candidates.append(candidate)
        
        # íŠ¹ë³„íˆ stream_dose_ai íŒ¨í„´ ìš°ì„  ì²˜ë¦¬
        special_patterns = [
            r'(stream_dose_ai_\d+)',
            r'(stream_does_ai_\d+)',
        ]
        
        for pattern in special_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if match not in filtered_candidates:
                    filtered_candidates.insert(0, match)  # ì•ì— ì¶”ê°€
        
        return list(set(filtered_candidates))  # ì¤‘ë³µ ì œê±°

    def create_drawing_search_prompt(self, user_question, search_results, rag_context):
        """ë„ë©´ ê²€ìƒ‰ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        drawing_search_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ë„ë©´ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- P&ID ë„ë©´ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë° ê´€ë¦¬
- ë„ë©´ ì •ë³´ ë¶„ì„ ë° ì¶”ì²œ
- ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë„ë©´ ì‹ë³„
- ë„ë©´ ë²„ì „ ê´€ë¦¬ ë° ì´ë ¥ ì¶”ì 

**ê²€ìƒ‰ ë¶„ì„ ì ‘ê·¼ë²•:**
1. **ê²€ìƒ‰ ê²°ê³¼ í‰ê°€**: ì°¾ì€ ë„ë©´ë“¤ì˜ ê´€ë ¨ì„± ë° ì í•©ì„± ë¶„ì„
2. **ë„ë©´ ì •ë³´ ì œê³µ**: ê° ë„ë©´ì˜ íŠ¹ì§•ê³¼ ë²„ì „ ì •ë³´ ì„¤ëª…
3. **ì¶”ì²œ ë° ì•ˆë‚´**: ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„ë©´ ì¶”ì²œ
4. **ì¶”ê°€ ì •ë³´**: í•„ìš”í•œ ê²½ìš° ê´€ë ¨ ë„ë©´ì´ë‚˜ ì¶”ê°€ ê²€ìƒ‰ ì œì•ˆ

**ë‹µë³€ êµ¬ì¡°:**
1. **ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½** (ì°¾ì€ ë„ë©´ ìˆ˜ì™€ ì£¼ìš” ê²°ê³¼)
2. **ë„ë©´ë³„ ìƒì„¸ ì •ë³´** (ì´ë¦„, ë²„ì „, ë“±ë¡ì, ë‚ ì§œ)
3. **ì¶”ì²œ ë„ë©´** (ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„ë©´)
4. **ì¶”ê°€ ì•ˆë‚´** (ê´€ë ¨ ì •ë³´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ)"""

        # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ êµ¬ì„±
        search_info = ""
        if search_results:
            search_info = f"**ê²€ìƒ‰ ê²°ê³¼ ({len(search_results)}ê°œ ë„ë©´ ë°œê²¬):**\n\n"
            for i, result in enumerate(search_results, 1):
                search_info += f"{i}. **{result['d_name']}**\n"
                search_info += f"   - ë²„ì „ ìˆ˜: {result['version_count']}ê°œ\n"
                search_info += f"   - ìµœì¢… ìˆ˜ì •: {result['latest_date']}\n"
                search_info += f"   - ë“±ë¡ì: {result['users']}\n\n"
        else:
            search_info = "**ê²€ìƒ‰ ê²°ê³¼:** ì¡°ê±´ì— ë§ëŠ” ë„ë©´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"

        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = f"""{drawing_search_persona}

**ì‚¬ìš©ì ì§ˆë¬¸:**
{user_question}

**ë„ë©´ ê²€ìƒ‰ ê²°ê³¼:**
{search_info}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

ì‚¬ìš©ìê°€ ë„ë©´ì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ë„ë©´ ê²€ìƒ‰ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

- ê²€ìƒ‰ëœ ë„ë©´ë“¤ì˜ íŠ¹ì§•ê³¼ ì°¨ì´ì 
- ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„ë©´ ì¶”ì²œ
- ê° ë„ë©´ì˜ ë²„ì „ ë° ì´ë ¥ ì •ë³´
- í•„ìš”í•œ ê²½ìš° ì¶”ê°€ ê²€ìƒ‰ì´ë‚˜ ê´€ë ¨ ë„ë©´ ì œì•ˆ

ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë„ë©´ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

        return system_prompt

    def get_drawing_data_by_id(self, d_id: int) -> Optional[Dict]:
        """
        d_idë¡œ íŠ¹ì • ë„ë©´ ë°ì´í„°ë¥¼ ì¡°íšŒ
        
        Args:
            d_id: ë„ë©´ ID
        
        Returns:
            ë„ë©´ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return None
            
            cursor = conn.cursor()
            
            query = """
            SELECT d_id, d_name, "user", create_date, json_data, image_path
            FROM domyun 
            WHERE d_id = %s
            """
            
            cursor.execute(query, (d_id,))
            result = cursor.fetchone()
            
            cursor.close()
            conn.close()
            
            if result:
                d_id, d_name, user, create_date, json_data, image_path = result
                return {
                    'd_id': d_id,
                    'd_name': d_name,
                    'user': user,
                    'create_date': create_date,
                    'json_data': json_data,
                    'image_path': image_path
                }
            else:
                logger.warning(f"ë„ë©´ ID '{d_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def search_drawings_by_name(self, search_term: str) -> List[Dict]:
        """
        ë„ë©´ ì´ë¦„ìœ¼ë¡œ LIKE ê²€ìƒ‰ì„ ìˆ˜í–‰
        
        Args:
            search_term: ê²€ìƒ‰í•  ë„ë©´ ì´ë¦„ (ë¶€ë¶„ ê²€ìƒ‰ ê°€ëŠ¥)
        
        Returns:
            ê²€ìƒ‰ëœ ë„ë©´ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ìµœì‹ ìˆœ)
        """
        try:
            conn = get_db_connection()
            if not conn:
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return []
            
            cursor = conn.cursor()
            
            # LIKE ê²€ìƒ‰ ì¿¼ë¦¬ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
            query = """
            SELECT DISTINCT d_name,
                   COUNT(*) as version_count,
                   MAX(create_date) as latest_date,
                   STRING_AGG(DISTINCT "user", ', ') as users
            FROM domyun 
            WHERE LOWER(d_name) LIKE LOWER(%s)
            GROUP BY d_name
            ORDER BY latest_date DESC
            """
            
            search_pattern = f"%{search_term}%"
            cursor.execute(query, (search_pattern,))
            results = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            drawings = []
            for d_name, version_count, latest_date, users in results:
                drawings.append({
                    'd_name': d_name,
                    'version_count': version_count,
                    'latest_date': latest_date,
                    'users': users
                })
            
            logger.info(f"ë„ë©´ ê²€ìƒ‰ '{search_term}': {len(drawings)}ê°œ ê²°ê³¼")
            return drawings
                
        except Exception as e:
            logger.error(f"ë„ë©´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def visualize_drawing_analysis(self, image_path: str, version: str = "latest") -> Optional[Dict]:
        """ë„ë©´ ì‹œê°í™” ë¶„ì„ ìˆ˜í–‰"""
        try:
            import os
            import json
            import base64
            from io import BytesIO

            visualizer = FirstDatasetVisualizer()
    
            # íŒŒì¼ ê²½ë¡œ ì„¤ì •
            png_path = "/Users/kjh/Desktop/doyeonso/doyeonso/uploads/uploaded_images/stream_dose_ai_1.png"
            json_path = "/Users/kjh/Desktop/doyeonso/doyeonso/uploads/merged_results/stream_dose_ai_1.json"
            
            # ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„±
            save_dir = "/Users/kjh/Desktop/doyeonso/doyeonso/uploads/uploaded_images"
            os.makedirs(save_dir, exist_ok=True)
            save_path = os.path.join(save_dir, "visualization_result.png")
            
            # JSON íŒŒì¼ì—ì„œ íƒì§€ëœ ê°ì²´ ì •ë³´ ì½ê¸°
            with open(json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # íƒì§€ëœ ê°ì²´ë“¤ì˜ ë¼ë²¨ê³¼ ID ì¶œë ¥
            print("\nğŸ” íƒì§€ëœ ê°ì²´ ëª©ë¡:")
            print("=" * 50)
            detected_objects = []
            for box in json_data.get('detecting', {}).get('data', {}).get('boxes', []):
                obj_info = f"ID: {box['id']} - {box['label']}"
                print(obj_info)
                detected_objects.append(obj_info)
            print("=" * 50)
            
            # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ì‹œê°í™” ì‹¤í–‰
            result_image = visualizer.visualize_dataset1(png_path, json_path, save_path, show_legend=True)
            
            if result_image:
                # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
                buffered = BytesIO()
                result_image.save(buffered, format="PNG")
                img_str = base64.b64encode(buffered.getvalue()).decode()
                
                # OCR ë°ì´í„° ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                ocr_data = json_data.get('ocr', {})
                ocr_images = ocr_data.get('images', [])
                ocr_fields = ocr_images[0].get('fields', []) if ocr_images else []
                ocr_count = len(ocr_fields)

                # Detection ë°ì´í„° ì ‘ê·¼
                detection_data = json_data.get('detecting', {}).get('data', {})
                detection_count = len(detection_data.get('boxes', []))

                # ë¶„ì„ ìš”ì•½ ìƒì„±
                analysis_summary = {
                    "total_objects": len(detected_objects),
                    "detected_objects": detected_objects,
                    "image_size": {
                        "width": json_data.get('width'),
                        "height": json_data.get('height')
                    },
                    "ocr_text_count": ocr_count
                }
                
                print(f"ğŸ’¾ ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ë¨: {save_path}")
                
                # OCR í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
                ocr_preview = []
                if 'ocr' in json_data and json_data['ocr']:
                    ocr_data = json_data['ocr']
                    if isinstance(ocr_data, dict) and 'images' in ocr_data:
                        for img in ocr_data['images']:
                            if 'fields' in img:
                                for field in img['fields']:
                                    if 'inferText' in field and field['inferText']:
                                        ocr_preview.append(field['inferText'])
                
                # Detection ê°ì²´ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
                detection_preview = []
                if 'detecting' in json_data and json_data['detecting']:
                    detection_data = json_data['detecting']
                    if isinstance(detection_data, dict) and 'data' in detection_data and 'boxes' in detection_data['data']:
                        for box in detection_data['data']['boxes']:
                            if isinstance(box, dict) and 'label' in box:
                                detection_preview.append(f"ID: {box.get('id', 'N/A')} - {box['label']}")
                
                # ì‹œê°í™” ê²°ê³¼ ë°˜í™˜
                return {
                    "success": True,
                    "image": img_str,
                    "save_path": save_path,
                    "analysis_summary": analysis_summary,
                    "ocr_count": ocr_count,
                    "detection_count": detection_count,
                    "drawing_name": os.path.basename(image_path),
                    "original_size": (json_data.get('width'), json_data.get('height')),
                    "resized_size": result_image.size,
                    "json_data": json_data,  # JSON ë°ì´í„° ì¶”ê°€
                    "ocr_preview": " | ".join(ocr_preview),  # OCR ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
                    "detection_preview": " | ".join(detection_preview)  # Detection ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
                }
            
            return None

        except Exception as e:
            logger.error(f"ë„ë©´ ì‹œê°í™” ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _draw_ocr_results(self, draw: ImageDraw.Draw, ocr_data: Dict, font, small_font) -> int:
        """
        OCR ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°
        
        Args:
            draw: PIL ImageDraw ê°ì²´
            ocr_data: OCR ë°ì´í„°
            font: í° í°íŠ¸
            small_font: ì‘ì€ í°íŠ¸
        
        Returns:
            ê·¸ë ¤ì§„ OCR í…ìŠ¤íŠ¸ ê°œìˆ˜
        """
        count = 0
        
        if 'images' not in ocr_data:
            return count
        
        for image_data in ocr_data['images']:
            if 'fields' not in image_data:
                continue
                
            for field in image_data['fields']:
                try:
                    # í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    infer_text = field.get('inferText', '')
                    if not infer_text:
                        continue
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    bounding_poly = field.get('boundingPoly')
                    if not bounding_poly or 'vertices' not in bounding_poly:
                        continue
                    
                    vertices = bounding_poly['vertices']
                    if len(vertices) < 4:
                        continue
                    
                    # ì¢Œí‘œ ì¶”ì¶œ (ì™¼ìª½ ìœ„, ì˜¤ë¥¸ìª½ ì•„ë˜)
                    x_coords = [v.get('x', 0) for v in vertices]
                    y_coords = [v.get('y', 0) for v in vertices]
                    
                    x1, y1 = min(x_coords), min(y_coords)
                    x2, y2 = max(x_coords), max(y_coords)
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰)
                    draw.rectangle([x1, y1, x2, y2], outline='blue', width=2)
                    
                    # í…ìŠ¤íŠ¸ ë ˆì´ë¸” ê·¸ë¦¬ê¸°
                    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                    display_text = infer_text[:20] + "..." if len(infer_text) > 20 else infer_text
                    
                    # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
                    text_bbox = draw.textbbox((x1, y1-15), display_text, font=small_font)
                    draw.rectangle(text_bbox, fill='blue')
                    draw.text((x1, y1-15), display_text, fill='white', font=small_font)
                    
                    count += 1
                    
                except Exception as e:
                    logger.warning(f"OCR í•„ë“œ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
                    continue
        
        return count

    def _draw_detection_results(self, draw: ImageDraw.Draw, detection_data: Dict, font) -> int:
        """Detection ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        count = 0
        
        if not detection_data or 'data' not in detection_data:
            return count
        
        for detection in detection_data['data']:
            try:
                # ë¼ë²¨ ì •ë³´
                label = detection.get('label', 'Unknown')
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ (ì¤‘ì‹¬ì  ê¸°ë°˜)
                bbox = detection.get('boxes')
                if not bbox:
                    continue
                
                center_x = bbox.get('x', 0)
                center_y = bbox.get('y', 0)
                width = bbox.get('width', 0)
                height = bbox.get('height', 0)
                
                # ì‹¤ì œ ì¢Œí‘œ ê³„ì‚° (ì¤‘ì‹¬ì ì—ì„œ ì¢Œìƒë‹¨, ìš°í•˜ë‹¨ ì¢Œí‘œë¡œ)
                x1 = center_x - width / 2
                y1 = center_y - height / 2
                x2 = center_x + width / 2
                y2 = center_y + height / 2
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ë¹¨ê°„ìƒ‰)
                draw.rectangle([x1, y1, x2, y2], outline='red', width=3)
                
                # ë ˆì´ë¸” í‘œì‹œ
                label_text = label
                
                # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
                text_bbox = draw.textbbox((x1, y1-20), label_text, font=font)
                draw.rectangle(text_bbox, fill='red')
                draw.text((x1, y1-20), label_text, fill='white', font=font)
                
                count += 1
                
            except Exception as e:
                logger.warning(f"Detection ê°ì²´ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")
                continue
        
        return count

    def analyze_drawing_with_visualization(self, d_name: str, user_question: str = None) -> Dict:
        """ë„ë©´ ë¶„ì„ê³¼ ì‹œê°í™”ë¥¼ í†µí•©í•˜ì—¬ ìˆ˜í–‰"""
        try:
            # ë„ë©´ ì‹œê°í™” ìˆ˜í–‰
            viz_result = self.visualize_drawing_analysis(d_name)
            
            if not viz_result:
                return {
                    'response': f"âŒ '{d_name}' ë„ë©´ì˜ ì‹œê°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    'sources': [],
                    'query_type': 'drawing_visualization',
                    'context_quality': 'none',
                    'web_search_used': False,
                    'visualization': None
                }
            
            # ì‹œê°í™” ê²°ê³¼ë§Œ ë°˜í™˜
            return {
                'response': viz_result['analysis_summary'],
                'sources': [{
                    'type': 'visualization',
                    'icon': 'ğŸ¨',
                    'source': f'ë„ë©´ ì‹œê°í™” - {d_name}',
                    'score': None,
                    'page': None,
                    'content_preview': f"OCR {viz_result['ocr_count']}ê°œ, Detection {viz_result['detection_count']}ê°œ ì‹œê°í™”",
                    'quality': 'high'
                }],
                'query_type': 'drawing_visualization',
                'context_quality': 'high',
                'web_search_used': False,
                'visualization': viz_result
            }
            
        except Exception as e:
            logger.error(f"ë„ë©´ ì‹œê°í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'response': f"ë„ë©´ ì‹œê°í™” ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'query_type': 'drawing_visualization',
                'context_quality': 'none',
                'web_search_used': False,
                'visualization': None
            }

    def compare_and_visualize_changes(self) -> Optional[Dict]:
        """
        stream_dose_ai_1.jsonê³¼ stream_dose_ai_3.jsonì„ ë¹„êµí•˜ì—¬ ë³€ê²½ëœ ë¶€ë¶„ë§Œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œí•˜ëŠ” ì‹œê°í™”
        
        Returns:
            ë³€ê²½ ë¹„êµ ì‹œê°í™” ê²°ê³¼
        """
        # ê³ ì •ëœ íŒŒì¼ ê²½ë¡œ
        as_is_json_path = "uploads/detection_results/stream_dose_ai_1.json"
        to_be_json_path = "uploads/detection_results/stream_dose_ai_3.json"
        as_is_image_path = "uploads/uploaded_images/stream_dose_ai_1.png"
        to_be_image_path = "uploads/uploaded_images/stream_dose_ai_3.png"
        
        try:
            # JSON íŒŒì¼ë“¤ ë¡œë“œ
            with open(as_is_json_path, 'r', encoding='utf-8') as f:
                as_is_data = json.load(f)
            
            with open(to_be_json_path, 'r', encoding='utf-8') as f:
                to_be_data = json.load(f)
            
            # ì´ë¯¸ì§€ ì¡´ì¬ í™•ì¸
            if not os.path.exists(as_is_image_path) or not os.path.exists(to_be_image_path):
                logger.error(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {as_is_image_path}, {to_be_image_path}")
                return None
            
            # ë³€ê²½ì‚¬í•­ ë¶„ì„
            changes = self._analyze_detection_changes(as_is_data, to_be_data)
            
            # as-is ì´ë¯¸ì§€ ì‹œê°í™”
            as_is_result = self._visualize_comparison_image(
                as_is_image_path, as_is_data, changes['removed'], 
                "AS-IS (ì œê±°ëœ ê°ì²´)", "red"
            )
            
            # to-be ì´ë¯¸ì§€ ì‹œê°í™”  
            to_be_result = self._visualize_comparison_image(
                to_be_image_path, to_be_data, changes['added'], 
                "TO-BE (ì¶”ê°€ëœ ê°ì²´)", "red"
            )
            
            # ë³€ê²½ í†µê³„
            change_stats = {
                'total_as_is': len(as_is_data['data']['boxes']),
                'total_to_be': len(to_be_data['data']['boxes']),
                'removed_count': len(changes['removed']),
                'added_count': len(changes['added']),
                'unchanged_count': len(changes['unchanged']),
                'modified_count': len(changes['modified'])
            }
            
            result = {
                'as_is_image': as_is_result,
                'to_be_image': to_be_result,
                'changes': changes,
                'statistics': change_stats,
                'analysis_summary': self._generate_change_summary(changes, change_stats)
            }
            
            logger.info(f"ë³€ê²½ ë¹„êµ ì™„ë£Œ: ì œê±° {change_stats['removed_count']}ê°œ, ì¶”ê°€ {change_stats['added_count']}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"ë³€ê²½ ë¹„êµ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return None

    def _analyze_detection_changes(self, as_is_data: Dict, to_be_data: Dict) -> Dict:
        """
        Detection ë°ì´í„°ì˜ ë³€ê²½ì‚¬í•­ì„ ë¶„ì„
        
        Args:
            as_is_data: as-is JSON ë°ì´í„°
            to_be_data: to-be JSON ë°ì´í„°
        
        Returns:
            ë³€ê²½ì‚¬í•­ ë¶„ì„ ê²°ê³¼
        """
        as_is_boxes = {box['id']: box for box in as_is_data['data']['boxes']}
        to_be_boxes = {box['id']: box for box in to_be_data['data']['boxes']}
        
        as_is_ids = set(as_is_boxes.keys())
        to_be_ids = set(to_be_boxes.keys())
        
        # ë³€ê²½ ìœ í˜•ë³„ ë¶„ë¥˜
        removed_ids = as_is_ids - to_be_ids  # as-isì—ë§Œ ìˆìŒ (ì œê±°ë¨)
        added_ids = to_be_ids - as_is_ids    # to-beì—ë§Œ ìˆìŒ (ì¶”ê°€ë¨)
        common_ids = as_is_ids & to_be_ids   # ë‘˜ ë‹¤ ìˆìŒ
        
        # ê³µí†µ ID ì¤‘ì—ì„œ ë³€ê²½ëœ ê²ƒê³¼ ë³€ê²½ë˜ì§€ ì•Šì€ ê²ƒ êµ¬ë¶„
        modified_ids = set()
        unchanged_ids = set()
        
        for obj_id in common_ids:
            as_is_box = as_is_boxes[obj_id]
            to_be_box = to_be_boxes[obj_id]
            
            # ìœ„ì¹˜ë‚˜ ë¼ë²¨ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if (as_is_box['label'] != to_be_box['label'] or
                abs(float(as_is_box['x']) - float(to_be_box['x'])) > 5 or
                abs(float(as_is_box['y']) - float(to_be_box['y'])) > 5 or
                abs(float(as_is_box['width']) - float(to_be_box['width'])) > 5 or
                abs(float(as_is_box['height']) - float(to_be_box['height'])) > 5):
                modified_ids.add(obj_id)
            else:
                unchanged_ids.add(obj_id)
        
        return {
            'removed': [as_is_boxes[obj_id] for obj_id in removed_ids],
            'added': [to_be_boxes[obj_id] for obj_id in added_ids],
            'modified': {
                'as_is': [as_is_boxes[obj_id] for obj_id in modified_ids],
                'to_be': [to_be_boxes[obj_id] for obj_id in modified_ids]
            },
            'unchanged': [as_is_boxes[obj_id] for obj_id in unchanged_ids]
        }

    def _visualize_comparison_image(self, image_path: str, json_data: Dict, 
                                  highlight_objects: List[Dict], title: str, 
                                  highlight_color: str = "red") -> Dict:
        """
        ë¹„êµ ì‹œê°í™”ë¥¼ ìœ„í•œ ì´ë¯¸ì§€ ì²˜ë¦¬
        
        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            json_data: JSON ë°ì´í„°
            highlight_objects: ê°•ì¡°í•  ê°ì²´ ë¦¬ìŠ¤íŠ¸
            title: ì´ë¯¸ì§€ ì œëª©
            highlight_color: ê°•ì¡° ìƒ‰ìƒ
        
        Returns:
            ì‹œê°í™”ëœ ì´ë¯¸ì§€ ì •ë³´
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            original_image = Image.open(image_path)
            
            # JSONì—ì„œ ì˜ˆìƒ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            expected_width = json_data['data'].get('width', original_image.width)
            expected_height = json_data['data'].get('height', original_image.height)
            
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            if original_image.size != (expected_width, expected_height):
                image = original_image.resize((expected_width, expected_height), Image.Resampling.LANCZOS)
            else:
                image = original_image.copy()
            
            # ê·¸ë¦¬ê¸° ê°ì²´ ìƒì„±
            draw = ImageDraw.Draw(image)
            
            # í°íŠ¸ ì„¤ì •
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 12)
                title_font = ImageFont.truetype("/System/Library/Fonts/Arial.ttf", 16)
            except:
                font = ImageFont.load_default()
                title_font = ImageFont.load_default()
            
            # ëª¨ë“  ê°ì²´ë¥¼ íšŒìƒ‰ìœ¼ë¡œ ê·¸ë¦¬ê¸° (ë°°ê²½)
            for box in json_data['data']['boxes']:
                self._draw_detection_box(draw, box, "lightgray", font, width=1)
            
            # ê°•ì¡°í•  ê°ì²´ë“¤ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ê·¸ë¦¬ê¸°
            highlight_ids = {obj['id'] for obj in highlight_objects}
            for box in json_data['data']['boxes']:
                if box['id'] in highlight_ids:
                    self._draw_detection_box(draw, box, highlight_color, font, width=3)
            
            # ì œëª© ì¶”ê°€
            draw.text((10, 10), title, fill="black", font=title_font)
            
            # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
            img_buffer = io.BytesIO()
            image.save(img_buffer, format='PNG', quality=95)
            img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
            
            return {
                'title': title,
                'image_base64': img_base64,
                'original_size': original_image.size,
                'resized_size': (expected_width, expected_height),
                'highlight_count': len(highlight_objects),
                'total_objects': len(json_data['data']['boxes'])
            }
            
        except Exception as e:
            logger.error(f"ë¹„êµ ì´ë¯¸ì§€ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            return None

    def _draw_detection_box(self, draw: ImageDraw.Draw, box: Dict, color: str, font, width: int = 2):
        """
        Detection ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ëŠ” í—¬í¼ í•¨ìˆ˜
        
        Args:
            draw: PIL ImageDraw ê°ì²´
            box: ë°•ìŠ¤ ì •ë³´
            color: ìƒ‰ìƒ
            font: í°íŠ¸
            width: ì„  ë‘ê»˜
        """
        try:
            # ì¢Œí‘œ ì •ë³´
            x = float(box['x'])
            y = float(box['y'])
            w = float(box['width'])
            h = float(box['height'])
            
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            draw.rectangle([x, y, x + w, y + h], outline=color, width=width)
            
            # ë¼ë²¨ ê·¸ë¦¬ê¸°
            label = box.get('label', 'Unknown')
            obj_id = box.get('id', '')
            text = f"{obj_id}: {label}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½
            if color != "lightgray":  # ê°•ì¡° ê°ì²´ë§Œ ë¼ë²¨ í‘œì‹œ
                text_bbox = draw.textbbox((x, y-20), text, font=font)
                draw.rectangle(text_bbox, fill=color)
                draw.text((x, y-20), text, fill='white', font=font)
                
        except Exception as e:
            logger.warning(f"Detection ë°•ìŠ¤ ê·¸ë¦¬ê¸° ì‹¤íŒ¨: {e}")

    def _generate_change_summary(self, changes: Dict, stats: Dict) -> str:
        """
        ë³€ê²½ì‚¬í•­ ìš”ì•½ ìƒì„±
        
        Args:
            changes: ë³€ê²½ì‚¬í•­ ë°ì´í„°
            stats: í†µê³„ ì •ë³´
        
        Returns:
            ë³€ê²½ì‚¬í•­ ìš”ì•½ í…ìŠ¤íŠ¸
        """
        summary = f"""ğŸ“Š **ë³€ê²½ì‚¬í•­ ë¶„ì„ ê²°ê³¼**

**ğŸ“ˆ ì „ì²´ í†µê³„:**
- AS-IS ì´ ê°ì²´: {stats['total_as_is']}ê°œ
- TO-BE ì´ ê°ì²´: {stats['total_to_be']}ê°œ
- ë³€ê²½ë˜ì§€ ì•ŠìŒ: {stats['unchanged_count']}ê°œ
- ìˆ˜ì •ë¨: {stats['modified_count']}ê°œ
- ì œê±°ë¨: {stats['removed_count']}ê°œ
- ì¶”ê°€ë¨: {stats['added_count']}ê°œ

**ğŸ”´ ì œê±°ëœ ê°ì²´ ({stats['removed_count']}ê°œ):**"""
        
        if changes['removed']:
            for obj in changes['removed']:
                summary += f"\n- ID {obj['id']}: {obj['label']} (ìœ„ì¹˜: {obj['x']}, {obj['y']})"
        else:
            summary += "\n- ì—†ìŒ"
        
        summary += f"\n\n**ğŸŸ¢ ì¶”ê°€ëœ ê°ì²´ ({stats['added_count']}ê°œ):**"
        
        if changes['added']:
            for obj in changes['added']:
                summary += f"\n- ID {obj['id']}: {obj['label']} (ìœ„ì¹˜: {obj['x']}, {obj['y']})"
        else:
            summary += "\n- ì—†ìŒ"
        
        if stats['modified_count'] > 0:
            summary += f"\n\n**ğŸŸ¡ ìˆ˜ì •ëœ ê°ì²´ ({stats['modified_count']}ê°œ):**"
            for as_is_obj, to_be_obj in zip(changes['modified']['as_is'], changes['modified']['to_be']):
                summary += f"\n- ID {as_is_obj['id']}: {as_is_obj['label']} â†’ {to_be_obj['label']}"
                if as_is_obj['label'] != to_be_obj['label']:
                    summary += f" (ë¼ë²¨ ë³€ê²½)"
                if (abs(float(as_is_obj['x']) - float(to_be_obj['x'])) > 5 or 
                    abs(float(as_is_obj['y']) - float(to_be_obj['y'])) > 5):
                    summary += f" (ìœ„ì¹˜ ë³€ê²½: {as_is_obj['x']},{as_is_obj['y']} â†’ {to_be_obj['x']},{to_be_obj['y']})"
        
        return summary

    def analyze_drawing_changes(self, user_question: str = None) -> Dict:
        """
        ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ë° ì‹œê°í™”ë¥¼ í†µí•©í•˜ì—¬ ìˆ˜í–‰
        
        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸ (ì„ íƒì‚¬í•­)
        
        Returns:
            ë³€ê²½ë¶„ì„ ê²°ê³¼ì™€ ì‹œê°í™” ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì‘ë‹µ
        """
        try:
            # ë³€ê²½ì‚¬í•­ ë¹„êµ ë° ì‹œê°í™” ìˆ˜í–‰
            comparison_result = self.compare_and_visualize_changes()
            
            if not comparison_result:
                return {
                    'response': "âŒ ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    'sources': [],
                    'query_type': 'change_analysis',
                    'context_quality': 'none',
                    'web_search_used': False,
                    'visualization': None
                }
            
            # AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            analysis_prompt = f"""ë‹¹ì‹ ì€ P&ID ë„ë©´ ë³€ê²½ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ P&ID ë„ë©´ì˜ AI íƒì§€ ê²°ê³¼ ë³€ê²½ì‚¬í•­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ğŸ”„ P&ID ë„ë©´ AI íƒì§€ ë³€ê²½ì‚¬í•­ í†µê³„:**
{comparison_result['analysis_summary']}

**ì¤‘ìš”:** ë¶„ì„ ë°ì´í„°ëŠ” ë‘ P&ID ë„ë©´ì—ì„œ AIê°€ ìë™ìœ¼ë¡œ íƒì§€í•œ ë‹¤ìŒ ìš”ì†Œë“¤ì˜ ë³€ê²½ì‚¬í•­ì…ë‹ˆë‹¤:
- **P&ID ê¸°í˜¸**: ê³„ì¸¡ê¸°ê¸°, ë°¸ë¸Œ, íŒí”„, íƒ±í¬, ì—´êµí™˜ê¸° ë“±ì˜ ê³µì • ê¸°í˜¸
- **í…ìŠ¤íŠ¸ ë¼ë²¨**: ê³„ì¸¡ê¸° íƒœê·¸ëª…(FT-101, PT-201 ë“±), ì„¤ë¹„ëª…, ë°°ê´€ ë²ˆí˜¸ ë“±
- **ì œì–´ ìš”ì†Œ**: ì œì–´ ë£¨í”„, ì¸í„°ë¡, ì•ˆì „ì¥ì¹˜ ë“±

**ë¶„ì„ ìš”ì²­:**
1. **ë³€ê²½ì‚¬í•­ ê°œìš”**: ì „ì²´ì ì¸ ë³€ê²½ì˜ ì„±ê²©ê³¼ ê³µì • ê°œì„  ëª©ì  ì¶”ì •
2. **ì œê±°ëœ P&ID ìš”ì†Œ ë¶„ì„**: ì œê±°ëœ ê³„ì¸¡ê¸°ê¸°ë‚˜ ê¸°í˜¸ê°€ ê³µì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. **ì¶”ê°€ëœ P&ID ìš”ì†Œ ë¶„ì„**: ìƒˆë¡œ ì¶”ê°€ëœ ê³„ì¸¡ê¸°ê¸°ë‚˜ ê¸°í˜¸ì˜ ì—­í• ê³¼ ëª©ì 
4. **ê³µì • ì•ˆì „ì„± ì˜í–¥**: ë³€ê²½ì´ ê³µì • ì•ˆì „ì„± ë° ì œì–´ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
5. **ìš´ì „ ì ˆì°¨ ì˜í–¥**: ë³€ê²½ì´ ê³µì • ìš´ì „ ë° ìœ ì§€ë³´ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
6. **ì—”ì§€ë‹ˆì–´ë§ ê¶Œì¥ì‚¬í•­**: P&ID ë³€ê²½ì‚¬í•­ ê²€í†  ì‹œ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•  ê¸°ìˆ ì  ì‚¬í•­

{"**ì‚¬ìš©ì ì§ˆë¬¸:** " + user_question if user_question else ""}

ì‹œê°í™”ëœ ì´ë¯¸ì§€ì—ì„œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œëœ ë¶€ë¶„ì´ AIê°€ íƒì§€í•œ ë³€ê²½ëœ P&ID ê¸°í˜¸ ë° í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤."""

            # OpenAI API í˜¸ì¶œ
            if not self.client:
                ai_response = "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            else:
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë„ë©´ ë³€ê²½ì‚¬í•­ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì•ˆì „ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ê²€í† í•©ë‹ˆë‹¤."},
                            {"role": "user", "content": analysis_prompt}
                        ],
                        temperature=0.2,
                        max_tokens=2000
                    )
                    
                    ai_response = response.choices[0].message.content
                    
                except Exception as e:
                    logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    ai_response = f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
            # ìµœì¢… ì‘ë‹µ êµ¬ì„±
            final_response = f"""ğŸ”„ **ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ê²°ê³¼**

**ğŸ“Š AI íƒì§€ ë³€ê²½ í†µê³„:**
- **AS-IS (stream_dose_ai_1)**: {comparison_result['statistics']['total_as_is']}ê°œ P&ID ê¸°í˜¸/í…ìŠ¤íŠ¸
- **TO-BE (stream_dose_ai_3)**: {comparison_result['statistics']['total_to_be']}ê°œ P&ID ê¸°í˜¸/í…ìŠ¤íŠ¸
- **ì œê±°ëœ ìš”ì†Œ**: {comparison_result['statistics']['removed_count']}ê°œ (ë¹¨ê°„ìƒ‰ í‘œì‹œ)
- **ì¶”ê°€ëœ ìš”ì†Œ**: {comparison_result['statistics']['added_count']}ê°œ (ë¹¨ê°„ìƒ‰ í‘œì‹œ)

---

{ai_response}

---

**ğŸ“Œ AI íƒì§€ ë³€ê²½ì‚¬í•­ ì‹œê°í™” ë²”ë¡€:**
- ğŸ”´ **ë¹¨ê°„ìƒ‰ ë°•ìŠ¤**: AIê°€ íƒì§€í•œ ë³€ê²½ëœ P&ID ê¸°í˜¸ ë° í…ìŠ¤íŠ¸ (ì œê±°/ì¶”ê°€)
- âšª **íšŒìƒ‰ ë°•ìŠ¤**: ë³€ê²½ë˜ì§€ ì•Šì€ P&ID ê¸°í˜¸ ë° í…ìŠ¤íŠ¸

**ğŸ“‹ ìƒì„¸ ë³€ê²½ ë‚´ì—­:**
{comparison_result['analysis_summary']}
"""
            
            # ì†ŒìŠ¤ ì •ë³´ êµ¬ì„±
            sources = [{
                'type': 'change_analysis',
                'icon': 'ğŸ”„',
                'source': 'ë„ë©´ ë³€ê²½ì‚¬í•­ ë¹„êµ ë¶„ì„',
                'score': None,
                'page': None,
                'content_preview': f"AS-IS vs TO-BE ë¹„êµ: ì œê±° {comparison_result['statistics']['removed_count']}ê°œ, ì¶”ê°€ {comparison_result['statistics']['added_count']}ê°œ",
                'quality': 'high'
            }]
            
            return {
                'response': final_response,
                'sources': sources,
                'query_type': 'change_analysis',
                'context_quality': 'high',
                'web_search_used': False,
                'visualization': comparison_result,
                'change_statistics': comparison_result['statistics']
            }
            
        except Exception as e:
            logger.error(f"ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'response': f"ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'query_type': 'change_analysis',
                'context_quality': 'none',
                'web_search_used': False,
                'visualization': None
            }

    def _extract_ocr_texts(self, json_data: Dict) -> List[str]:
        """JSON ë°ì´í„°ì—ì„œ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        ocr_texts = []
        
        try:
            # ìƒˆë¡œìš´ êµ¬ì¡° ('ocr')
            if 'ocr' in json_data and json_data['ocr']:
                ocr_data = json_data['ocr']
                if 'images' in ocr_data:
                    for image in ocr_data['images']:
                        if 'fields' in image:
                            for field in image['fields']:
                                if 'inferText' in field and field['inferText']:
                                    ocr_texts.append(field['inferText'])
            # ì´ì „ êµ¬ì¡° ('ocr_data')
            elif 'ocr_data' in json_data and json_data['ocr_data']:
                ocr_data = json_data['ocr_data']
                if 'images' in ocr_data:
                    for image in ocr_data['images']:
                        if 'fields' in image:
                            for field in image['fields']:
                                if 'inferText' in field and field['inferText']:
                                    ocr_texts.append(field['inferText'])
        except Exception as e:
            logger.error(f"OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return ocr_texts

    def _extract_detection_info(self, json_data: Dict) -> List[Dict]:
        """Detection ì •ë³´ ì¶”ì¶œ"""
        detection_info = []
        
        # ìƒˆë¡œìš´ í˜•ì‹ (detection_data)
        if 'detection_data' in json_data and isinstance(json_data['detection_data'], dict):
            detections = json_data['detection_data'].get('detections', [])
            if isinstance(detections, list):
                for detection in detections:
                    if isinstance(detection, dict):
                        info = {
                            'label': detection.get('label', 'Unknown'),
                            'id': detection.get('id', ''),
                        }
                        
                        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                        if 'boundingBox' in detection:
                            bbox = detection['boundingBox']
                            info['x'] = bbox.get('x', 0)
                            info['y'] = bbox.get('y', 0)
                            info['width'] = bbox.get('width', 0)
                            info['height'] = bbox.get('height', 0)
                        elif all(k in detection for k in ['x', 'y', 'width', 'height']):
                            info['x'] = detection['x']
                            info['y'] = detection['y']
                            info['width'] = detection['width']
                            info['height'] = detection['height']
                        
                        detection_info.append(info)
        
        # ì´ì „ í˜•ì‹ (detecting)
        elif 'detecting' in json_data and isinstance(json_data['detecting'], dict):
            data = json_data['detecting'].get('data', {})
            if isinstance(data, dict) and 'boxes' in data:
                boxes = data['boxes']
                if isinstance(boxes, list):
                    for box in boxes:
                        if isinstance(box, dict):
                            info = {
                                'label': box.get('label', 'Unknown'),
                                'id': box.get('id', ''),
                            }
                            
                            # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                            if all(k in box for k in ['x', 'y', 'width', 'height']):
                                info['x'] = box['x']
                                info['y'] = box['y']
                                info['width'] = box['width']
                                info['height'] = box['height']
                            
                            detection_info.append(info)
        
        # ì§ì ‘ boxes ë°°ì—´ì´ ìˆëŠ” ê²½ìš°
        elif isinstance(json_data, dict) and 'boxes' in json_data:
            boxes = json_data['boxes']
            if isinstance(boxes, list):
                for box in boxes:
                    if isinstance(box, dict):
                        info = {
                            'label': box.get('label', 'Unknown'),
                            'id': box.get('id', ''),
                        }
                        
                        # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
                        if all(k in box for k in ['x', 'y', 'width', 'height']):
                            info['x'] = box['x']
                            info['y'] = box['y']
                            info['width'] = box['width']
                            info['height'] = box['height']
                        
                        detection_info.append(info)
        
        return detection_info