#!/usr/bin/env python3
"""
P&ID ì „ë¬¸ê°€ ì±—ë´‡ ëª¨ë¸ - Streamlit ì—°ë™ìš©
"""

import os
import pickle
import torch
from sentence_transformers import SentenceTransformer
from utils.rag_system_kiwi import RAGSystemWithKiwi
from openai import OpenAI
from loguru import logger
from typing import List, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class PIDExpertChatbot:
    """P&ID ë„ë©´ ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡"""
    
    def __init__(self):
        """ì±—ë´‡ ì´ˆê¸°í™”"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            self.client = OpenAI(api_key=self.openai_api_key)
        else:
            self.client = None
            logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.kiwi_rag = RAGSystemWithKiwi()
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
        self.chunks = None
        self.embeddings = None
        
        # ëŒ€í™” ê¸°ë¡
        self.conversation_history = []
        
        # ìƒíƒœ ì €ì¥ ê²½ë¡œ
        self.STATE_PATH = "./state/"
        
        # ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ì •ì˜
        self.expert_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID(Piping and Instrumentation Diagram) ì „ë¬¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- í™”í•™ê³µì • ì„¤ê³„ ë° ì œì–´ì‹œìŠ¤í…œ
- ê³„ì¸¡ê¸°ê¸° ë° ì œì–´ë£¨í”„ ë¶„ì„
- ê³µì • ì•ˆì „ ë° ì´ìƒìƒí™© ëŒ€ì‘
- P&ID ë„ë©´ í•´ì„ ë° ê²€í† 

**ì‘ë‹µ ìŠ¤íƒ€ì¼:**
- ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì¡°ì–¸ ì œê³µ
- ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤
- êµ¬ì²´ì ì¸ ê³„ì¸¡ê¸°ê¸° íƒœê·¸(FT, FC, PT ë“±) ì–¸ê¸‰
- í•„ìš”ì‹œ ì¶”ê°€ ê²€í† ì‚¬í•­ ì œì•ˆ

**ì‘ë‹µ êµ¬ì¡°:**
1. í•µì‹¬ ë‹µë³€ (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)
2. ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ (ê´€ë ¨ ê³„ì¸¡ê¸°ê¸°, ì œì–´ë¡œì§ ë“±)
3. ì•ˆì „ ê³ ë ¤ì‚¬í•­ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
4. ì¶”ê°€ ê¶Œì¥ì‚¬í•­ (í•„ìš”í•œ ê²½ìš°)"""

    def save_state(self, chunks, embeddings):
        """ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ì €ì¥"""
        os.makedirs(self.STATE_PATH, exist_ok=True)
        try:
            with open(os.path.join(self.STATE_PATH, "state.pkl"), "wb") as f:
                pickle.dump({"chunks": chunks, "embeddings": embeddings}, f)
            logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_state(self):
        """ì €ì¥ëœ ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ë¡œë“œ"""
        state_file = os.path.join(self.STATE_PATH, "state.pkl")
        if os.path.exists(state_file):
            try:
                with open(state_file, "rb") as f:
                    data = pickle.load(f)
                    return data["chunks"], data["embeddings"]
            except Exception as e:
                logger.error(f"ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

    def create_embeddings(self, chunks):
        """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ì—¬ í…ì„œë¡œ ë³€í™˜"""
        embeddings = self.embedder.encode(chunks, convert_to_tensor=True)
        return embeddings

    def initialize_rag_system(self, pdf_path: str) -> bool:
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
            
            # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹œë„
            chunks, embeddings = self.load_state()
            
            if chunks is None or embeddings is None:
                logger.info("ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
                
                # PDF ë¡œë“œ ë° ì²­í‚¹
                if not os.path.exists(pdf_path):
                    logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
                    return False
                
                documents = self.kiwi_rag.extract_text_from_pdf(pdf_path)
                
                if not documents:
                    logger.error("PDF ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨")
                    return False
                
                # ì²­í‚¹
                chunks_metadata = self.kiwi_rag.chunk_documents_with_kiwi(documents)
                chunks = [chunk['content'] for chunk in chunks_metadata]
                
                # ì„ë² ë”© ìƒì„±
                logger.info("ì„ë² ë”© ìƒì„± ì¤‘...")
                embeddings = self.create_embeddings(chunks)
                
                # ìƒíƒœ ì €ì¥
                self.save_state(chunks, embeddings)
            else:
                logger.info("ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            
            self.chunks = chunks
            self.embeddings = embeddings
            
            logger.info(f"RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {embeddings.shape} ì„ë² ë”©")
            return True
            
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def retrieve_relevant_chunks(self, query, top_k=3):
        """RAG ê²€ìƒ‰ - ê´€ë ¨ ì²­í¬ ì¶”ì¶œ"""
        if self.chunks is None or self.embeddings is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedder.encode(query, convert_to_tensor=True)
            query_embedding = query_embedding / torch.norm(query_embedding)
            
            # ì„ë² ë”© ì •ê·œí™”
            embeddings_norm = self.embeddings / torch.norm(self.embeddings, dim=1, keepdim=True)
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.matmul(embeddings_norm, query_embedding)
            
            # ìƒìœ„ kê°œ ì¶”ì¶œ
            top_k_indices = torch.topk(similarities, min(top_k, len(similarities))).indices.tolist()
            top_k_scores = torch.topk(similarities, min(top_k, len(similarities))).values.tolist()
            
            # ê´€ë ¨ ì²­í¬ì™€ ì ìˆ˜ ë°˜í™˜
            relevant_chunks = []
            for i, (idx, score) in enumerate(zip(top_k_indices, top_k_scores)):
                relevant_chunks.append({
                    'content': self.chunks[idx],
                    'score': score,
                    'rank': i + 1,
                    'page': i + 1  # Streamlit í˜¸í™˜ì„±ì„ ìœ„í•´ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                })
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def build_rag_context(self, relevant_chunks):
        """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        context = ""
        for chunk in relevant_chunks:
            context += f"[ì°¸ê³ ìë£Œ {chunk['rank']}] (ìœ ì‚¬ë„: {chunk['score']:.3f})\n"
            context += f"{chunk['content']}\n\n"
        return context.strip()

    def create_pid_expert_prompt(self, user_question, rag_context):
        """P&ID ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        system_prompt = f"""{self.expert_persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ P&ID ì§€ì‹ì„ í™œìš©í•˜ë˜, ì¶”ì¸¡ì´ ì•„ë‹Œ í™•ì‹¤í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”."""

        return system_prompt

    def _detect_query_type(self, query: str) -> str:
        """ì¿¼ë¦¬ ìœ í˜• ê°ì§€ - ë³€ê²½ ë¶„ì„ ê°•í™”"""
        
        # ë³€ê²½/ë¹„êµ ê´€ë ¨ í‚¤ì›Œë“œ (ë” ì •í™•í•œ ê°ì§€)
        change_keywords = [
            'ë³€ê²½', 'ì°¨ì´', 'ë¹„êµ', 'ìˆ˜ì •', 'ê°œì„ ', 'ì—…ë°ì´íŠ¸', 'ë°”ë€', 'ë‹¬ë¼ì§„',
            'ì´ì „', 'ê¸°ì¡´', 'ì›ë˜', 'ìƒˆë¡œìš´', 'ë³€í™”', 'ë‹¤ë¥¸ì ', 'ì°¨ì´ì ',
            'ê°œì •', 'ìˆ˜ì •ì‚¬í•­', 'ë³€ê²½ì‚¬í•­', 'ì—…ê·¸ë ˆì´ë“œ', 'êµì²´', 'êµí™˜',
            'ì „í›„', 'ë³€ë™', 'ì¡°ì •', 'ê°œëŸ‰', 'ê°œì„ ì‚¬í•­'
        ]
        
        # ë¹„êµ í‘œí˜„ íŒ¨í„´
        comparison_patterns = [
            r'vs|versus',  # A vs B
            r'ì™€\s*ë¹„êµ',  # Aì™€ ë¹„êµ
            r'ê³¼\s*ë¹„êµ',  # Aê³¼ ë¹„êµ
            r'ëŒ€ë¹„',       # A ëŒ€ë¹„ B
            r'ì—ì„œ\s*.*ë¡œ',  # Aì—ì„œ Bë¡œ
            r'ê¸°ì¡´.*ìƒˆë¡œìš´',  # ê¸°ì¡´ A ìƒˆë¡œìš´ B
            r'ì´ì „.*í˜„ì¬',   # ì´ì „ A í˜„ì¬ B
        ]
        
        safety_keywords = ['ì•ˆì „', 'ìœ„í—˜', 'ë¹„ìƒ', 'ì •ì§€', 'ë³´í˜¸', 'ì•ŒëŒ', 'ESD', 'SIS', 'ì¸í„°ë¡']
        instrument_keywords = ['FT', 'FC', 'FV', 'PT', 'PC', 'TT', 'TC', 'LT', 'LC', 'AT', 'AC', 'ê³„ì¸¡ê¸°', 'íŠ¸ëœìŠ¤ë¯¸í„°', 'ì¡°ì ˆê¸°']
        
        query_lower = query.lower()
        
        # ë³€ê²½/ë¹„êµ í‚¤ì›Œë“œ ìš°ì„  í™•ì¸
        if any(keyword in query for keyword in change_keywords):
            return "change_analysis"
        
        # ë¹„êµ íŒ¨í„´ í™•ì¸
        import re
        for pattern in comparison_patterns:
            if re.search(pattern, query):
                return "change_analysis"
        
        # ê¸°íƒ€ ë¶„ë¥˜
        if any(keyword in query for keyword in safety_keywords):
            return "safety_analysis"
        elif any(keyword in query for keyword in instrument_keywords):
            return "instrument_explanation"
        else:
            return "general"

    def create_change_analysis_prompt(self, user_question, rag_context):
        """ë³€ê²½ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        change_expert_persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID ë³€ê²½ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- P&ID ë„ë©´ ë³€ê²½ì‚¬í•­ ë¶„ì„ ë° ê²€í† 
- ê³µì • ê°œì„  ë° ì„¤ë¹„ êµì²´ ì˜í–¥ ë¶„ì„
- ë³€ê²½ ì „í›„ ë¹„êµ ë¶„ì„
- ë³€ê²½ì‚¬í•­ì˜ ì•ˆì „ì„± ë° ìš´ì „ì„± í‰ê°€

**ë³€ê²½ ë¶„ì„ ì ‘ê·¼ë²•:**
1. **ë³€ê²½ ì‚¬í•­ ì‹ë³„**: ì •í™•íˆ ë¬´ì—‡ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ íŒŒì•…
2. **ì˜í–¥ë„ ë¶„ì„**: ë³€ê²½ì´ ì£¼ë³€ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. **ì•ˆì „ì„± ê²€í† **: ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì•ˆì „ ë¦¬ìŠ¤í¬ í‰ê°€
4. **ìš´ì „ì„± ê²€í† **: ìš´ì „ ì ˆì°¨ ë° ìœ ì§€ë³´ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
5. **ê¶Œì¥ì‚¬í•­**: ë³€ê²½ ì‹œ ê³ ë ¤í•´ì•¼ í•  ì¶”ê°€ ì‚¬í•­

**ë‹µë³€ êµ¬ì¡°:**
1. **ë³€ê²½ì‚¬í•­ ìš”ì•½** (ë¬´ì—‡ì´ ì–´ë–»ê²Œ ë°”ë€Œì—ˆëŠ”ì§€)
2. **ê¸°ìˆ ì  ì˜í–¥ ë¶„ì„** (ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥)
3. **ì•ˆì „ì„± ê²€í† ** (ì•ˆì „ ê´€ë ¨ ê³ ë ¤ì‚¬í•­)
4. **ìš´ì „ ë° ìœ ì§€ë³´ìˆ˜ ì˜í–¥** (ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­)
5. **ê¶Œì¥ì‚¬í•­ ë° ì£¼ì˜ì‚¬í•­** (ì¶”ê°€ ê²€í†  í•„ìš” ì‚¬í•­)"""

        system_prompt = f"""{change_expert_persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

ì‚¬ìš©ìê°€ P&ID ë³€ê²½ ë˜ëŠ” ë¹„êµì— ëŒ€í•´ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ë³€ê²½ ê´€ë¦¬ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

- ë³€ê²½ì‚¬í•­ì˜ êµ¬ì²´ì  ë‚´ìš©ê³¼ ë²”ìœ„
- ì—°ê´€ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥
- ì•ˆì „ì„± ë° ìš´ì „ì„± ê´€ì ì—ì„œì˜ ê²€í† 
- ë³€ê²½ ì‹œ ì¶”ê°€ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ì²´ê³„ì ì¸ ë³€ê²½ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        return system_prompt

    def retrieve_change_analysis_chunks(self, query, top_k=5):
        """ë³€ê²½ ë¶„ì„ì„ ìœ„í•œ í™•ì¥ëœ ê²€ìƒ‰"""
        if self.chunks is None or self.embeddings is None:
            return []
        
        try:
            # ê¸°ë³¸ ê²€ìƒ‰
            basic_chunks = self.retrieve_relevant_chunks(query, top_k=3)
            
            # ë³€ê²½ ê´€ë ¨ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
            change_terms = ['ë³€ê²½', 'ìˆ˜ì •', 'ê°œì„ ', 'êµì²´', 'ì—…ê·¸ë ˆì´ë“œ', 'ì¡°ì •']
            additional_chunks = []
            
            for term in change_terms:
                if term not in query:  # ì´ë¯¸ í¬í•¨ëœ ìš©ì–´ëŠ” ì œì™¸
                    expanded_query = f"{query} {term}"
                    extra_chunks = self.retrieve_relevant_chunks(expanded_query, top_k=2)
                    additional_chunks.extend(extra_chunks)
            
            # ì¤‘ë³µ ì œê±° ë° í†µí•©
            all_chunks = basic_chunks + additional_chunks
            seen_contents = set()
            unique_chunks = []
            
            for chunk in all_chunks:
                if chunk['content'] not in seen_contents:
                    seen_contents.add(chunk['content'])
                    unique_chunks.append(chunk)
            
            # ìƒìœ„ top_kê°œ ë°˜í™˜
            return unique_chunks[:top_k]
            
        except Exception as e:
            logger.error(f"ë³€ê²½ ë¶„ì„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self.retrieve_relevant_chunks(query, top_k)

    def generate_response(self, user_query: str, use_web_search: bool = False) -> Dict:
        """ì±—ë´‡ ì‘ë‹µ ìƒì„± - ë³€ê²½ ë¶„ì„ ë¶„ê¸° ì²˜ë¦¬"""
        try:
            # ì¿¼ë¦¬ ìœ í˜• ê°ì§€
            query_type = self._detect_query_type(user_query)
            
            # ë³€ê²½ ë¶„ì„ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
            if query_type == "change_analysis":
                logger.info(f"ğŸ”„ ë³€ê²½ ë¶„ì„ ëª¨ë“œë¡œ ì²˜ë¦¬: {user_query}")
                
                # í™•ì¥ëœ ê²€ìƒ‰ ìˆ˜í–‰
                relevant_chunks = self.retrieve_change_analysis_chunks(user_query, top_k=5)
                rag_context = self.build_rag_context(relevant_chunks)
                
                # ë³€ê²½ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                system_prompt = self.create_change_analysis_prompt(user_query, rag_context)
                
                # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ max_tokens ì¦ê°€
                max_tokens = 2000
                temperature = 0.2  # ë” ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
                
            else:
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                relevant_chunks = self.retrieve_relevant_chunks(user_query, top_k=3)
                rag_context = self.build_rag_context(relevant_chunks)
                system_prompt = self.create_pid_expert_prompt(user_query, rag_context)
                max_tokens = 1500
                temperature = 0.3
            
            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸
            context_quality = "high" if len(rag_context) > 100 else "low"
            
            # OpenAI API í˜¸ì¶œ
            if not self.client:
                return {
                    'response': "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    'sources': [],
                    'query_type': query_type,
                    'context_quality': context_quality,
                    'web_search_used': False
                }
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_query}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                ai_response = response.choices[0].message.content
                
                # ë³€ê²½ ë¶„ì„ì¸ ê²½ìš° ì‘ë‹µì— íŠ¹ë³„ í‘œì‹œ ì¶”ê°€
                if query_type == "change_analysis":
                    ai_response = "ğŸ”„ **ë³€ê²½ ë¶„ì„ ëª¨ë“œ**\n\n" + ai_response
                
            except Exception as e:
                logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                ai_response = f"OpenAI API ì˜¤ë¥˜: {e}"
            
            # ì†ŒìŠ¤ ì •ë³´ ìƒì„± (Streamlit í˜¸í™˜)
            sources = []
            for chunk in relevant_chunks:
                sources.append({
                    'page': chunk['page'],
                    'score': chunk['score'],
                    'content_preview': chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content']
                })
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.conversation_history.append({
                'timestamp': datetime.now(),
                'user_query': user_query,
                'response': ai_response,
                'query_type': query_type,
                'context_quality': context_quality,
                'sources_count': len(sources)
            })
            
            return {
                'response': ai_response,
                'sources': sources,
                'query_type': query_type,
                'context_quality': context_quality,
                'web_search_used': use_web_search
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'response': f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'query_type': 'error',
                'context_quality': 'none',
                'web_search_used': False
            }

    def get_conversation_summary(self) -> Dict:
        """ëŒ€í™” ìš”ì•½ í†µê³„"""
        if not self.conversation_history:
            return {}
        
        total_queries = len(self.conversation_history)
        query_types = {}
        context_qualities = {}
        
        for conv in self.conversation_history:
            query_type = conv.get('query_type', 'unknown')
            context_quality = conv.get('context_quality', 'unknown')
            
            query_types[query_type] = query_types.get(query_type, 0) + 1
            context_qualities[context_quality] = context_qualities.get(context_quality, 0) + 1
        
        return {
            'total_queries': total_queries,
            'query_types': query_types,
            'context_qualities': context_qualities,
            'average_sources_per_query': sum(conv.get('sources_count', 0) for conv in self.conversation_history) / total_queries
        }

    def clear_conversation_history(self):
        """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
        self.conversation_history = []
        logger.info("ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def export_conversation_history(self) -> str:
        """ëŒ€í™” ê¸°ë¡ ë‚´ë³´ë‚´ê¸°"""
        if not self.conversation_history:
            return "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
        
        export_text = "P&ID ì „ë¬¸ê°€ ì±—ë´‡ ëŒ€í™” ê¸°ë¡\n"
        export_text += "=" * 50 + "\n\n"
        
        for i, conv in enumerate(self.conversation_history, 1):
            export_text += f"ëŒ€í™” {i}\n"
            export_text += f"ì‹œê°„: {conv['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\n"
            export_text += f"ì§ˆë¬¸: {conv['user_query']}\n"
            export_text += f"ë‹µë³€: {conv['response']}\n"
            export_text += f"ì¿¼ë¦¬ ìœ í˜•: {conv['query_type']}\n"
            export_text += f"ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ: {conv['context_quality']}\n"
            export_text += "-" * 30 + "\n\n"
        
        return export_text 