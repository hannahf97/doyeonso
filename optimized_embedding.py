#!/usr/bin/env python3
"""
ìµœì í™”ëœ ì„ë² ë”© ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ
"""

import torch
import numpy as np
import os
import pickle
from sentence_transformers import SentenceTransformer
from utils.rag_system_kiwi import RAGSystemWithKiwi

# ì‚¬ìš©ì ì œê³µ ì½”ë“œ ê¸°ë°˜
embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

STATE_PATH = "./state/"

def save_state(chunks, embeddings):
    """ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ì €ì¥"""
    os.makedirs(STATE_PATH, exist_ok=True)  # state ë””ë ‰í† ë¦¬ ìƒì„±
    with open(os.path.join(STATE_PATH, "state.pkl"), "wb") as f:  # 'state.pkl' íŒŒì¼ì„ ì“°ê¸° ëª¨ë“œë¡œ ì—½ë‹ˆë‹¤.
        pickle.dump({"chunks": chunks, "embeddings": embeddings}, f)  # ì²­í¬ì™€ ì„ë² ë”©ì„ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    print(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì™„ë£Œ: {STATE_PATH}state.pkl")

def load_state():
    """ì €ì¥ëœ ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ë¡œë“œ"""
    state_file = os.path.join(STATE_PATH, "state.pkl")
    if os.path.exists(state_file):
        with open(state_file, "rb") as f:
            data = pickle.load(f)
            print(f"ğŸ“‚ ìƒíƒœ ë¡œë“œ ì™„ë£Œ: {len(data['chunks'])}ê°œ ì²­í¬, {data['embeddings'].shape} ì„ë² ë”©")
            return data["chunks"], data["embeddings"]
    else:
        print(f"âŒ ì €ì¥ëœ ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {state_file}")
        return None, None

def create_embeddings(chunks):
    """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ì—¬ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    embeddings = embedder.encode(chunks, convert_to_tensor=True)  # ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ì—¬ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    return embeddings  # ì„ë² ë”©ëœ ë²¡í„°ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

def create_normalized_embeddings(chunks):
    """ì •ê·œí™”ëœ ì„ë² ë”© ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”)"""
    embeddings = create_embeddings(chunks)
    # L2 ì •ê·œí™”ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ìµœì í™”
    normalized_embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)
    return normalized_embeddings

def retrieve_relevant_chunks(query, chunks, embeddings, top_k=5):
    """
    ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ê²€ìƒ‰ í•¨ìˆ˜
    """
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embedder.encode(query, convert_to_tensor=True)
    query_embedding = query_embedding / torch.norm(query_embedding)
    
    # ì„ë² ë”© ì •ê·œí™” (ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš° ìƒëµ ê°€ëŠ¥)
    embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    similarities = torch.matmul(embeddings, query_embedding.T).squeeze()
    
    # ìƒìœ„ kê°œ ì¶”ì¶œ
    top_k_indices = torch.topk(similarities, top_k).indices.tolist()
    
    # ê´€ë ¨ ì²­í¬ ë°˜í™˜
    relevant_chunks = [chunks[i] for i in top_k_indices]
    
    return '\n\n'.join(relevant_chunks)

def build_complete_vector_db():
    """ì™„ì „í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (pickle ì €ì¥/ë¡œë“œ ì§€ì›)"""
    
    print("ğŸ—ï¸ ì™„ì „í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•")
    print("=" * 60)
    
    # ê¸°ì¡´ ìƒíƒœ í™•ì¸ ë° ë¡œë“œ ì‹œë„
    chunks, embeddings = load_state()
    
    if chunks is not None and embeddings is not None:
        print("âœ… ê¸°ì¡´ ì €ì¥ëœ ìƒíƒœë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!")
        return chunks, embeddings, None
    
    # ìƒˆë¡œ êµ¬ì¶•í•˜ëŠ” ê²½ìš°
    print("ğŸ”„ ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
    
    # PDF ë¡œë“œ
    pdf_path = "data/ê³µì • Description_ê¸€.pdf"
    
    # Kiwi ì‹œìŠ¤í…œ ì‚¬ìš© (ë” ë‚˜ì€ ì²­í‚¹)
    kiwi_rag = RAGSystemWithKiwi()
    documents = kiwi_rag.extract_text_from_pdf(pdf_path)
    
    if not documents:
        print("âŒ PDF ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨")
        return None, None, None
    
    # ì²­í‚¹
    chunks_metadata = kiwi_rag.chunk_documents_with_kiwi(documents)
    chunk_texts = [chunk['content'] for chunk in chunks_metadata]
    
    print(f"ğŸ“„ ì²­í‚¹ ì™„ë£Œ: {len(chunk_texts)}ê°œ ì²­í¬")
    
    # ì„ë² ë”© ìƒì„±
    print("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = create_normalized_embeddings(chunk_texts)
    
    # ìƒíƒœ ì €ì¥
    save_state(chunk_texts, embeddings)
    
    print(f"âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ:")
    print(f"  - ì²­í¬ ìˆ˜: {len(chunk_texts)}")
    print(f"  - ì„ë² ë”© shape: {embeddings.shape}")
    print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {embeddings.element_size() * embeddings.nelement() / 1024 / 1024:.2f} MB")
    
    return chunk_texts, embeddings, chunks_metadata

def interactive_search(chunks, embeddings):
    """ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤"""
    
    print("\nğŸ” ëŒ€í™”í˜• P&ID ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    
    while True:
        query = input("\nğŸ’¬ ì§ˆë¬¸: ").strip()
        
        if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not query:
            continue
        
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            result = retrieve_relevant_chunks(query, chunks, embeddings, top_k=3)
            
            print(f"\nğŸ“‹ ê²€ìƒ‰ ê²°ê³¼:")
            print("-" * 40)
            print(result)
            
            # ìœ ì‚¬ë„ ì ìˆ˜ë„ í‘œì‹œ
            query_embedding = embedder.encode(query, convert_to_tensor=True)
            query_embedding = query_embedding / torch.norm(query_embedding)
            similarities = torch.matmul(embeddings, query_embedding.T).squeeze()
            top_3_similarities = torch.topk(similarities, 3).values.tolist()
            
            print(f"\nğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {[f'{s:.3f}' for s in top_3_similarities]}")
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

def benchmark_search_performance(chunks, embeddings):
    """ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    print("\nâš¡ ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)
    
    import time
    
    test_queries = [
        "FT101ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‹œì•½ ì£¼ì… ë°©ë²•",
        "ë¹„ìƒìƒí™© ëŒ€ì‘ì ˆì°¨",
        "ìœ ëŸ‰ ì œì–´ ì‹œìŠ¤í…œ",
        "ì˜¨ë„ ì¸¡ì • ë°©ë²•",
        "ì••ë ¥ ì œì–´ ë°©ì‹",
        "ì•ˆì „ì¥ì¹˜ ì‘ë™ì›ë¦¬",
        "ë°°ê´€ ì„¤ê³„ íŠ¹ì§•",
        "ì œì–´ë£¨í”„ êµ¬ì„±",
        "ê³„ì¸¡ê¸°ê¸° ì¢…ë¥˜"
    ]
    
    print(f"ğŸ” {len(test_queries)}ê°œ ì¿¼ë¦¬ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    # ì„±ëŠ¥ ì¸¡ì •
    start_time = time.time()
    
    for i, query in enumerate(test_queries, 1):
        result = retrieve_relevant_chunks(query, chunks, embeddings, top_k=3)
        print(f"  ì¿¼ë¦¬ {i:2d}: {len(result)}ì ê²°ê³¼ ë°˜í™˜")
    
    end_time = time.time()
    
    total_time = end_time - start_time
    avg_time = total_time / len(test_queries)
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
    print(f"  - ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"  - í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ/ì¿¼ë¦¬")
    print(f"  - ì²˜ë¦¬ëŸ‰: {1/avg_time:.1f} ì¿¼ë¦¬/ì´ˆ")

def clear_state():
    """ì €ì¥ëœ ìƒíƒœ íŒŒì¼ ì‚­ì œ"""
    state_file = os.path.join(STATE_PATH, "state.pkl")
    if os.path.exists(state_file):
        os.remove(state_file)
        print(f"ğŸ—‘ï¸ ìƒíƒœ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {state_file}")
    else:
        print(f"âŒ ì‚­ì œí•  ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {state_file}")

def get_state_info():
    """ì €ì¥ëœ ìƒíƒœ íŒŒì¼ ì •ë³´ í™•ì¸"""
    state_file = os.path.join(STATE_PATH, "state.pkl")
    if os.path.exists(state_file):
        file_size = os.path.getsize(state_file)
        file_size_mb = file_size / 1024 / 1024
        print(f"ğŸ“Š ìƒíƒœ íŒŒì¼ ì •ë³´:")
        print(f"  - íŒŒì¼ ê²½ë¡œ: {state_file}")
        print(f"  - íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
        
        # ê°„ë‹¨í•œ ë‚´ìš© í™•ì¸
        try:
            with open(state_file, "rb") as f:
                data = pickle.load(f)
                print(f"  - ì²­í¬ ìˆ˜: {len(data['chunks'])}")
                print(f"  - ì„ë² ë”© shape: {data['embeddings'].shape}")
                print(f"  - ì„ë² ë”© íƒ€ì…: {type(data['embeddings'])}")
        except Exception as e:
            print(f"  - íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    else:
        print(f"âŒ ìƒíƒœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {state_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ìµœì í™”ëœ P&ID ì„ë² ë”© ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    # ìƒíƒœ ì •ë³´ í™•ì¸
    get_state_info()
    
    # ì‚¬ìš©ì ì˜µì…˜
    print("\nğŸ”§ ì˜µì…˜:")
    print("1. ë²¡í„° DB êµ¬ì¶• ë° ê²€ìƒ‰ ì‹œì‘")
    print("2. ìƒíƒœ íŒŒì¼ ì‚­ì œ í›„ ìƒˆë¡œ êµ¬ì¶•")
    print("3. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    if choice == "2":
        clear_state()
        print("ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
    elif choice == "3":
        print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    elif choice != "1":
        print("ê¸°ë³¸ê°’ìœ¼ë¡œ ë²¡í„° DB êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        chunks, embeddings, metadata = build_complete_vector_db()
        
        if chunks is None:
            return
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        benchmark_search_performance(chunks, embeddings)
        
        # ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œì‘
        interactive_search(chunks, embeddings)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 