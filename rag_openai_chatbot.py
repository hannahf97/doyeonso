#!/usr/bin/env python3
"""
RAG + OpenAI P&ID ì „ë¬¸ê°€ ì±—ë´‡
"""

import os
import pickle
import torch
from sentence_transformers import SentenceTransformer
from utils.rag_system_kiwi import RAGSystemWithKiwi
from openai import OpenAI
import json
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

STATE_PATH = "./state/"

def save_state(chunks, embeddings):
    """ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ì €ì¥"""
    os.makedirs(STATE_PATH, exist_ok=True)
    with open(os.path.join(STATE_PATH, "state.pkl"), "wb") as f:
        pickle.dump({"chunks": chunks, "embeddings": embeddings}, f)

def load_state():
    """ì €ì¥ëœ ì²­í¬ì™€ ì„ë² ë”©ì„ pickleë¡œ ë¡œë“œ"""
    state_file = os.path.join(STATE_PATH, "state.pkl")
    if os.path.exists(state_file):
        with open(state_file, "rb") as f:
            data = pickle.load(f)
            return data["chunks"], data["embeddings"]
    return None, None

def create_embeddings(chunks):
    """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ì—¬ í…ì„œë¡œ ë³€í™˜"""
    embeddings = embedder.encode(chunks, convert_to_tensor=True)
    return embeddings

def retrieve_relevant_chunks(query, chunks, embeddings, top_k=3):
    """RAG ê²€ìƒ‰ - ê´€ë ¨ ì²­í¬ ì¶”ì¶œ"""
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embedder.encode(query, convert_to_tensor=True)
    query_embedding = query_embedding / torch.norm(query_embedding)
    
    # ì„ë² ë”© ì •ê·œí™”
    embeddings_norm = embeddings / torch.norm(embeddings, dim=1, keepdim=True)
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    similarities = torch.matmul(embeddings_norm, query_embedding)
    
    # ìƒìœ„ kê°œ ì¶”ì¶œ
    top_k_indices = torch.topk(similarities, min(top_k, len(similarities))).indices.tolist()
    top_k_scores = torch.topk(similarities, min(top_k, len(similarities))).values.tolist()
    
    # ê´€ë ¨ ì²­í¬ì™€ ì ìˆ˜ ë°˜í™˜
    relevant_chunks = []
    for i, (idx, score) in enumerate(zip(top_k_indices, top_k_scores)):
        relevant_chunks.append({
            'content': chunks[idx],
            'score': score,
            'rank': i + 1
        })
    
    return relevant_chunks

def build_rag_context(relevant_chunks):
    """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
    context = ""
    for chunk in relevant_chunks:
        context += f"[ì°¸ê³ ìë£Œ {chunk['rank']}] (ìœ ì‚¬ë„: {chunk['score']:.3f})\n"
        context += f"{chunk['content']}\n\n"
    return context.strip()

def create_pid_expert_prompt(user_question, rag_context):
    """P&ID ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    persona = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ P&ID(Piping and Instrumentation Diagram) ì „ë¬¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- í™”í•™ê³µì • ì„¤ê³„ ë° ì œì–´ì‹œìŠ¤í…œ
- ê³„ì¸¡ê¸°ê¸° ë° ì œì–´ë£¨í”„ ë¶„ì„
- ê³µì • ì•ˆì „ ë° ì´ìƒìƒí™© ëŒ€ì‘
- P&ID ë„ë©´ í•´ì„ ë° ê²€í† 

**ì‘ë‹µ ìŠ¤íƒ€ì¼:**
- ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì ì¸ ì¡°ì–¸ ì œê³µ
- ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤
- êµ¬ì²´ì ì¸ ê³„ì¸¡ê¸°ê¸° íƒœê·¸(FT, FC, PT ë“±) ì–¸ê¸‰
- í•„ìš”ì‹œ ì¶”ê°€ ê²€í† ì‚¬í•­ ì œì•ˆ

**ì‘ë‹µ êµ¬ì¡°:**
1. í•µì‹¬ ë‹µë³€ (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)
2. ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ (ê´€ë ¨ ê³„ì¸¡ê¸°ê¸°, ì œì–´ë¡œì§ ë“±)
3. ì•ˆì „ ê³ ë ¤ì‚¬í•­ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
4. ì¶”ê°€ ê¶Œì¥ì‚¬í•­ (í•„ìš”í•œ ê²½ìš°)"""

    system_prompt = f"""{persona}

**ì°¸ê³  ë¬¸ì„œ ì •ë³´:**
{rag_context}

ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ P&ID ì§€ì‹ì„ í™œìš©í•˜ë˜, ì¶”ì¸¡ì´ ì•„ë‹Œ í™•ì‹¤í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”."""

    return system_prompt

def ask_openai_with_rag(user_question, rag_context, model="gpt-4o-mini", temperature=0.3):
    """OpenAI APIë¥¼ í†µí•´ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
    
    try:
        system_prompt = create_pid_expert_prompt(user_question, rag_context)
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_question}
            ],
            temperature=temperature,
            max_tokens=1500
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        return f"âŒ OpenAI API ì˜¤ë¥˜: {e}"

def setup_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    # ê¸°ì¡´ ìƒíƒœ ë¡œë“œ ì‹œë„
    chunks, embeddings = load_state()
    
    if chunks is None or embeddings is None:
        print("ğŸ“„ ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # PDF ë¡œë“œ ë° ì²­í‚¹
        pdf_path = "data/ê³µì • Description_ê¸€.pdf"
        kiwi_rag = RAGSystemWithKiwi()
        documents = kiwi_rag.extract_text_from_pdf(pdf_path)
        
        if not documents:
            print("âŒ PDF ë¬¸ì„œ ì¶”ì¶œ ì‹¤íŒ¨")
            return None, None
        
        # ì²­í‚¹
        chunks_metadata = kiwi_rag.chunk_documents_with_kiwi(documents)
        chunks = [chunk['content'] for chunk in chunks_metadata]
        
        # ì„ë² ë”© ìƒì„±
        print("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = create_embeddings(chunks)
        
        # ìƒíƒœ ì €ì¥
        save_state(chunks, embeddings)
        print("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    else:
        print("ğŸ“‚ ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
    
    print(f"âœ… RAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {embeddings.shape} ì„ë² ë”©")
    return chunks, embeddings

def interactive_pid_chatbot():
    """P&ID ì „ë¬¸ê°€ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤"""
    
    print("\nğŸ¤– P&ID ì „ë¬¸ê°€ ì±—ë´‡ ì‹œì‘")
    print("=" * 80)
    print("ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” P&ID ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤.")
    print("ê³µì •ë„ë©´, ê³„ì¸¡ê¸°ê¸°, ì œì–´ì‹œìŠ¤í…œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("-" * 80)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    chunks, embeddings = setup_rag_system()
    
    if chunks is None:
        print("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # ëŒ€í™” ê¸°ë¡
    conversation_history = []
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            user_question = input("\nğŸ’¬ ì§ˆë¬¸: ").strip()
            
            if user_question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ P&ID ì „ë¬¸ê°€ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆì „í•œ í•˜ë£¨ ë˜ì„¸ìš”!")
                break
            
            if not user_question:
                continue
            
            print("\nğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            # RAG ê²€ìƒ‰
            relevant_chunks = retrieve_relevant_chunks(user_question, chunks, embeddings, top_k=3)
            rag_context = build_rag_context(relevant_chunks)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(relevant_chunks)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
            for chunk in relevant_chunks:
                print(f"  â€¢ ìœ ì‚¬ë„ {chunk['score']:.3f}: {chunk['content'][:50]}...")
            
            print("\nğŸ¤– P&ID ì „ë¬¸ê°€ê°€ ë‹µë³€ ìƒì„± ì¤‘...")
            
            # OpenAI ë‹µë³€ ìƒì„±
            ai_response = ask_openai_with_rag(user_question, rag_context)
            
            # ë‹µë³€ ì¶œë ¥
            print("\n" + "="*80)
            print("ğŸ”§ P&ID ì „ë¬¸ê°€ ë‹µë³€:")
            print("-" * 80)
            print(ai_response)
            print("="*80)
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            conversation_history.append({
                'timestamp': datetime.now().isoformat(),
                'question': user_question,
                'rag_chunks': len(relevant_chunks),
                'rag_scores': [chunk['score'] for chunk in relevant_chunks],
                'response': ai_response
            })
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    if conversation_history:
        save_conversation_history(conversation_history)

def save_conversation_history(conversation_history):
    """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
    try:
        os.makedirs("./logs", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"./logs/conversation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(conversation_history, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ ëŒ€í™” ê¸°ë¡ ì €ì¥: {filename}")
        print(f"ì´ {len(conversation_history)}ê°œì˜ ì§ˆë¬¸-ë‹µë³€ ì €ì¥ë¨")
    except Exception as e:
        print(f"âŒ ëŒ€í™” ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")

def test_rag_openai_integration():
    """RAG + OpenAI í†µí•© í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª RAG + OpenAI í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    chunks, embeddings = setup_rag_system()
    
    if chunks is None:
        print("âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "FT101ì€ ë¬´ì—‡ì´ê³  ì–´ë–¤ ì—­í• ì„ í•˜ë‚˜ìš”?",
        "ì‹œì•½ ì£¼ì… ì‹œìŠ¤í…œì˜ ì œì–´ ë°©ì‹ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ë¹„ìƒìƒí™©ì—ì„œ ì–´ë–¤ ì•ˆì „ì¥ì¹˜ê°€ ì‘ë™í•˜ë‚˜ìš”?",
        "ì˜¨ë„ ì œì–´ëŠ” ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ {i}: {question}")
        print("-" * 40)
        
        # RAG ê²€ìƒ‰
        relevant_chunks = retrieve_relevant_chunks(question, chunks, embeddings, top_k=2)
        rag_context = build_rag_context(relevant_chunks)
        
        print(f"ğŸ” RAG ê²€ìƒ‰: {len(relevant_chunks)}ê°œ ê´€ë ¨ ë¬¸ì„œ")
        
        # OpenAI ë‹µë³€
        ai_response = ask_openai_with_rag(question, rag_context)
        
        print(f"ğŸ¤– AI ë‹µë³€:\n{ai_response}")
        print("\n" + "="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ RAG + OpenAI P&ID ì „ë¬¸ê°€ ì±—ë´‡")
    print("=" * 80)
    
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”:")
        print("export OPENAI_API_KEY='your-api-key-here'")
        return
    
    # ì‚¬ìš©ì ì„ íƒ
    print("\nğŸ”§ ì˜µì…˜:")
    print("1. ëŒ€í™”í˜• P&ID ì±—ë´‡ ì‹œì‘")
    print("2. RAG + OpenAI í†µí•© í…ŒìŠ¤íŠ¸")
    print("3. ì¢…ë£Œ")
    
    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
    
    try:
        if choice == "1":
            interactive_pid_chatbot()
        elif choice == "2":
            test_rag_openai_integration()
        elif choice == "3":
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            interactive_pid_chatbot()
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 